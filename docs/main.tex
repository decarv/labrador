% Próximos passos:
%
%   1. Terminar o componente de avaliação dos experimentos baseado em nDCG. (1a sem. Setembro)
%   2. Criar consultas e estabelecer o ranking de resultados para poder realizar experimentos preliminares. (1a sem. Setembro)
%   3. Rodar experimentos preliminares de relevância. (1a sem. Setembro)
%   4. Aprender sobre Índice Invertido e descrever sua implementação. (2-3a sem. Setembro)
%   5. Aprender sobre Indexação de Embeddings e descrever a implementação desses índices. (2-3a sem. Setembro)
%   6. Criar índices para LLM mais performático em (3). (4a sem. Setembro)
%   7. Criar índices para busca com BM25 (usar weaviate). (4a sem. Setembro)
%   8. Realizar experimentos com BM25 e LLMs. (4a sem. Setembro)
%   9. Melhorar o sistema de recuperação avaliando o resultado de 8 (1a sem. Setembro)
%   10. Subir o experimento e coletar consultas e rankings de documentos. (1a sem. Outubro)
%   11. Comparar o resultado do sistema de teses e do meu sistema para as consultas coletadas em 10 (2a sem. Outubro)
%   12. Terminar relatórios.


% ------------------------------------------------------------------------
% ------------------------------------------------------------------------
% abnTeX2: Modelo de Trabalho Academico (tese de doutorado, dissertacao de
% mestrado e trabalhos monograficos em geral) em conformidade com
% ABNT NBR 14724:2011: Informacao e documentacao - Trabalhos academicos -
% Apresentacao
% ------------------------------------------------------------------------
% ------------------------------------------------------------------------

\documentclass[
	% -- opções da classe memoir --
	12pt,				% tamanho da fonte
	openright,			% capítulos começam em pág ímpar (insere página vazia caso preciso)
	oneside,			% para impressão em verso e anverso. Oposto a oneside
	a4paper,			% tamanho do papel.
	% -- opções da classe abntex2 --
	%chapter=TITLE,		% títulos de capítulos convertidos em letras maiúsculas
	%section=TITLE,		% títulos de seções convertidos em letras maiúsculas
	%subsection=TITLE,	% títulos de subseções convertidos em letras maiúsculas
	%subsubsection=TITLE,% títulos de subsubseções convertidos em letras maiúsculas
	% -- opções do pacote babel --
	english,			% idioma adicional para hifenização
	french,				% idioma adicional para hifenização
	spanish,			% idioma adicional para hifenização
	brazil				% o último idioma é o principal do documento
	]{abntex2}

% ---
% Pacotes básicos
% ---
\usepackage{lmodern}			% Usa a fonte Latin Modern
\usepackage[T1]{fontenc}		% Selecao de codigos de fonte.
\usepackage[utf8]{inputenc}		% Codificacao do documento (conversão automática dos acentos)
\usepackage{lastpage}			% Usado pela Ficha catalográfica
\usepackage{indentfirst}		% Indenta o primeiro parágrafo de cada seção.
\usepackage{color}				% Controle das cores
\usepackage{graphicx}			% Inclusão de gráficos
\usepackage{microtype} 			% para melhorias de justificação
\usepackage{amsmath}
% ---

% ---
% Pacotes adicionais, usados apenas no âmbito do Modelo Canônico do abnteX2
% ---

% ---
% Pacotes de citações
% ---
\usepackage[brazilian,hyperpageref]{backref}	 % Paginas com as citações na bibl
\usepackage[num]{abntex2cite}
\citebrackets[]

% citar com
% \cite{}
% \citeonline{}
% \citeauthoronline{}
% \citeyear{}
% \citebrackets() é possível!

\usepackage{amsmath}
\usepackage{biblatex}	% Citações padrão ABNT

\addbibresource{books.bib} %Imports bibliography file
\addbibresource{articles.bib} %Imports bibliography file
\addbibresource{online.bib} %Imports bibliography file
\addbibresource{misc.bib} %Imports bibliography file
\addbibresource{inproceedings.bib} %Imports bibliography file
% \addbibresource{thesis.bib} %Imports bibliography file

% ---
% CONFIGURAÇÕES DE PACOTES
% ---

% ---
% Configurações do pacote backref
% Usado sem a opção hyperpageref de backref
\renewcommand{\backrefpagesname}{Citado na(s) página(s):~}
% Texto padrão antes do número das páginas
\renewcommand{\backref}{}
% Define os textos da citação
\renewcommand*{\backrefalt}[4]{
	\ifcase #1 %
		Nenhuma citação no texto.%
	\or
		Citado na página #2.%
	\else
		Citado #1 vezes nas páginas #2.%
	\fi}%
% ---

% ---
% Informações de dados para CAPA e FOLHA DE ROSTO
% ---
\titulo{Título}
\autor{Henrique Araújo de Carvalho}
\local{São Paulo, Brasil}
\data{2023}
\orientador{Daniel Macêdo Batista}
%\coorientador{Coorientador}
\instituicao{%
  Universidade de São Paulo
  \par
  Instituto de Matemática e Estatística
  \par
  Graduação}
\tipotrabalho{Trabalho de Formatura}
% O preambulo deve conter o tipo do trabalho, o objetivo,
% o nome da instituição e a área de concentração
%\preambulo{Preâmbulo.}
% ---

% ---
% Configurações de aparência do PDF final

% alterando o aspecto da cor azul
\definecolor{blue}{RGB}{41,5,195}

% informações do PDF
\makeatletter
\hypersetup{
     	%pagebackref=true,
		pdftitle={\@title},
		pdfauthor={\@author},
    	pdfsubject={\imprimirpreambulo},
	    pdfcreator={LaTeX with abnTeX2},
		pdfkeywords={abnt}{latex}{abntex}{abntex2}{trabalho acadêmico},
		colorlinks=true,       		% false: boxed links; true: colored links
    	linkcolor=blue,          	% color of internal links
    	citecolor=blue,        		% color of links to bibliography
    	filecolor=magenta,      		% color of file links
		urlcolor=blue,
		bookmarksdepth=4
}
\makeatother
% ---

% ---
% Espaçamentos entre linhas e parágrafos
% ---

% O tamanho do parágrafo é dado por:
\setlength{\parindent}{1.3cm}

% Controle do espaçamento entre um parágrafo e outro:
\setlength{\parskip}{0.2cm}  % tente também \onelineskip

% ---
% compila o indice
% ---
\makeindex
% ---

% ----
% Início do documento
% ----
\begin{document}

% Retira espaço extra obsoleto entre as frases.
\frenchspacing

% ----------------------------------------------------------
% ELEMENTOS PRÉ-TEXTUAIS
% ----------------------------------------------------------
% \pretextual

% ---
% Capa
% ---
\imprimircapa
% ---

% ---
% Folha de rosto
% (o * indica que haverá a ficha bibliográfica)
% ---
\imprimirfolhaderosto*
% ---

% ---
% Inserir a ficha bibliografica
% ---

% Isto é um exemplo de Ficha Catalográfica, ou ``Dados internacionais de
% catalogação-na-publicação''. Você pode utilizar este modelo como referência.
% Porém, provavelmente a biblioteca da sua universidade lhe fornecerá um PDF
% com a ficha catalográfica definitiva após a defesa do trabalho. Quando estiver
% com o documento, salve-o como PDF no diretório do seu projeto e substitua todo
% o conteúdo de implementação deste arquivo pelo comando abaixo:
%
% \begin{fichacatalografica}
%     \includepdf{fig_ficha_catalografica.pdf}
% \end{fichacatalografica}
\begin{fichacatalografica}
	\vspace*{\fill}					% Posição vertical
	\hrule							% Linha horizontal
	\begin{center}					% Minipage Centralizado
	\begin{minipage}[c]{12.5cm}		% Largura

	\imprimirautor

	\hspace{0.5cm} \imprimirtitulo  / \imprimirautor. --
	\imprimirlocal, \imprimirdata-

	\hspace{0.5cm} \pageref{LastPage} p. : il. (algumas color.) ; 30 cm.\\

	\hspace{0.5cm} \imprimirorientadorRotulo~\imprimirorientador\\

	\hspace{0.5cm}
	\parbox[t]{\textwidth}{\imprimirtipotrabalho~--~\imprimirinstituicao,
	\imprimirdata.}\\

	\hspace{0.5cm}
		1. Palavra-chave1.
		2. Palavra-chave2.
		I. Orientador.
		II. Universidade xxx.
		III. Faculdade de xxx.
		IV. Título\\

	\hspace{8.75cm} CDU 02:141:005.7\\

	\end{minipage}
	\end{center}
	\hrule
\end{fichacatalografica}
% ---

% % ---
% % Inserir errata
% % ---
% \begin{errata}
% Elemento opcional da \citeonline[4.2.1.2]{NBR14724:2011}. Exemplo:

% \vspace{\onelineskip}

% FERRIGNO, C. R. A. \textbf{Tratamento de neoplasias ósseas apendiculares com
% reimplantação de enxerto ósseo autólogo autoclavado associado ao plasma
% rico em plaquetas}: estudo crítico na cirurgia de preservação de membro em
% cães. 2011. 128 f. Tese (Livre-Docência) - Faculdade de Medicina Veterinária e
% Zootecnia, Universidade de São Paulo, São Paulo, 2011.

% \begin{table}[htb]
% \center
% \footnotesize
% \begin{tabular}{|p{1.4cm}|p{1cm}|p{3cm}|p{3cm}|}
%   \hline
%    \textbf{Folha} & \textbf{Linha}  & \textbf{Onde se lê}  & \textbf{Leia-se}  \\
%     \hline
%     1 & 10 & auto-conclavo & autoconclavo\\
%    \hline
% \end{tabular}
% \end{table}

% \end{errata}
% % ---

% ---
% Inserir folha de aprovação
% ---

% Isto é um exemplo de Folha de aprovação, elemento obrigatório da NBR
% 14724/2011 (seção 4.2.1.3). Você pode utilizar este modelo até a aprovação
% do trabalho. Após isso, substitua todo o conteúdo deste arquivo por uma
% imagem da página assinada pela banca com o comando abaixo:
%
% \includepdf{folhadeaprovacao_final.pdf}
% %
% \begin{folhadeaprovacao}

%   \begin{center}
%     {\ABNTEXchapterfont\large\imprimirautor}

%     \vspace*{\fill}\vspace*{\fill}
%     \begin{center}
%       \ABNTEXchapterfont\bfseries\Large\imprimirtitulo
%     \end{center}
%     \vspace*{\fill}

%     \hspace{.45\textwidth}
%     \begin{minipage}{.5\textwidth}
%         \imprimirpreambulo
%     \end{minipage}%
%     \vspace*{\fill}
%    \end{center}

%    Trabalho aprovado. \imprimirlocal, 24 de novembro de 2012:

%    \assinatura{\textbf{\imprimirorientador} \\ Orientador}
%    \assinatura{\textbf{Professor} \\ Convidado 1}
%    \assinatura{\textbf{Professor} \\ Convidado 2}
%    %\assinatura{\textbf{Professor} \\ Convidado 3}
%    %\assinatura{\textbf{Professor} \\ Convidado 4}

%    \begin{center}
%     \vspace*{0.5cm}
%     {\large\imprimirlocal}
%     \par
%     {\large\imprimirdata}
%     \vspace*{1cm}
%   \end{center}

% \end{folhadeaprovacao}
% % ---

% % ---
% % Dedicatória
% % ---
% \begin{dedicatoria}
%    \vspace*{\fill}
%    \centering
%    \noindent
%    \textit{ Este trabalho é dedicado às crianças adultas que,\\
%    quando pequenas, sonharam em se tornar cientistas.} \vspace*{\fill}
% \end{dedicatoria}
% % ---

% ---
% Agradecimentos
% ---
\begin{agradecimentos}
Agradecimentos.
\end{agradecimentos}
% ---

% % ---
% % Epígrafe
% % ---
% \begin{epigrafe}
%     \vspace*{\fill}
% 	\begin{flushright}
% 		\textit{``Não vos amoldeis às estruturas deste mundo, \\
% 		mas transformai-vos pela renovação da mente, \\
% 		a fim de distinguir qual é a vontade de Deus: \\
% 		o que é bom, o que Lhe é agradável, o que é perfeito.\\
% 		(Bíblia Sagrada, Romanos 12, 2)}
% 	\end{flushright}
% \end{epigrafe}
% % ---

% ---
% RESUMOS
% ---

% resumo em português
\setlength{\absparsep}{18pt} % ajusta o espaçamento dos parágrafos do resumo
\begin{resumo}
 Segundo a \citeonline[3.1-3.2]{NBR6028:2003}, o resumo deve ressaltar o
 objetivo, o método, os resultados e as conclusões do documento. A ordem e a extensão
 destes itens dependem do tipo de resumo (informativo ou indicativo) e do
 tratamento que cada item recebe no documento original. O resumo deve ser
 precedido da referência do documento, com exceção do resumo inserido no
 próprio documento. (\ldots) As palavras-chave devem figurar logo abaixo do
 resumo, antecedidas da expressão Palavras-chave:, separadas entre si por
 ponto e finalizadas também por ponto.

 \textbf{Palavras-chaves}: latex. abntex. editoração de texto.
\end{resumo}

% resumo em inglês
\begin{resumo}[Abstract]
 \begin{otherlanguage*}{english}
   This is the english abstract.

   \vspace{\onelineskip}

   \noindent
   \textbf{Key-words}: latex. abntex. text editoration.
 \end{otherlanguage*}
\end{resumo}

% ---
% inserir lista de ilustrações
% ---
%\pdfbookmark[0]{\listfigurename}{lof}
%\listoffigures*
%\cleardoublepage
% ---
% ---
% inserir lista de tabelas
% ---
%\pdfbookmark[0]{\listtablename}{lot}
%\listoftables*
%\cleardoublepage
% ---
% ---
% inserir lista de abreviaturas e siglas
% ---
%\begin{siglas}
%  \item[ABNT] Associação Brasileira de Normas Técnicas
%  \item[abnTeX] ABsurdas Normas para TeX
%\end{siglas}
% ---
% ---
% inserir lista de símbolos
% ---
%\begin{simbolos}
%  \item[$ \Gamma $] Letra grega Gama
%  \item[$ \Lambda $] Lambda
%  \item[$ \zeta $] Letra grega minúscula zeta
%  \item[$ \in $] Pertence
%\end{simbolos}
% ---
% ---
% inserir o sumario
% ---
\pdfbookmark[0]{\contentsname}{toc}
\tableofcontents*
\cleardoublepage
% ---

% ----------------------------------------------------------
% ELEMENTOS TEXTUAIS
% ----------------------------------------------------------
\textual

\chapter*[Introdução]{Introdução}
\addcontentsline{toc}{chapter}{Introdução}
\label{chap:intro}

% EM CONSTRUÇÃO -------------------------------------------------------------------------------------------------------------------------



O propósito desse trabalho é desenvolver um sistema de recuperação de trabalhos acadêmicos, com ênfase em técnicas modernas
de recuperação desse tipo de documento.

Descrevo o desenvolvimento desse sistema comparando duas técnicas para recuperação e ranqueamento de documentos.
A primeira técnica, baseada em busca por palavras-chave, é a base da implementação de sistemas de recuperação de informação tradicionais.
A segunda técnica, baseada em busca por \textit{embeddings} (``EBR -- Embeddings Based Search''), é como a implementação de novos sistemas de recuperação
tem sido feita recentemente.

Minha contribuição com o presente trabalho é traçar as bases da criação de um sistema moderno de recuperação de trabalhos acadêmicos nacionais.
A forma que utilizo para medir o sucesso do meu sistema é compará-lo com o sistema de busca por palavras-chave da Biblioteca Digital
de Teses e Dissertações da Universidade de São Paulo \footnote{https://www.teses.usp.br/}.

No capítulo \ref{ch:recuperacao-de-informacao} discuto ...


\chapter{Referencial Teórico}\label{ch:sistemas-de-recuperacao-de-informacao}

\section{Recuperação de Informação}\label{sec:sobre-recuperacao-de-informacao}

Recuperação de informação (``IR -- Information Retrieval'') pode assumir diferentes definições.
Para este trabalho, IR é definida como a disciplina que estuda como encontrar documentos relevantes, de natureza desestruturada, em meio a uma coleção de documentos, a
partir de uma consulta. \cite{manning2008introduction}

Documento, no contexto da IR, referencia qualquer pedaço de informação que se deseja recuperar.
No caso deste trabalho, documentos são os títulos, resumos e palavras-chave de teses acadêmicas, já que planeja-se usar esses dados para encontrar os trabalhos relevantes.
Presume-se que se um documento é relevante, o trabalho associado a este documento também é relevante.

Documentos relevantes são aqueles que satisfazem a necessidade do usuário. \cite{manning2008introduction}
A definição de relevância é específica de um problema e depende de fatores como
características da busca, do usuário a que ela se destina, da consulta, dos documentos, e de
tempo e espaço da busca, o que confere à relevância uma natureza subjetiva e dinâmica.
Como explicam \citeauthor{Ceri2013}, ``a relevância é multifacetada, sendo determinada não só pelo conteúdo de um
resultado recuperado, mas também por aspectos como autoridade, credibilidade, especificidade, exaustividade,
atualidade e clareza de sua fonte''.

Documentos são de natureza desestruturada quando não possuem um modelo de dados definido, ou que são difíceis para um computador
estruturar, interpretar, analisar ou segmentar. \cite{manning2008introduction}
Exemplos de dados desestruturados são texto, imagem, áudio e vídeo, que são encontrados na forma de
documentos como páginas da web, artigos científicos, imagens, filmes, dentre outros formatos.
Em contraste, documentos estruturados são os que possuem uma estrutura relacional clara ou são fáceis para um computador estruturar.
A disciplina de estudo de dados estruturados é a recuperação de dados, que se baseia em uma forma expressa e bem
definida de recuperação, articulada de maneira formal, sobre informação que possui um modelo pré-definido de dados. \cite{JurafskyMartin2023}

A IR é tratada como um campo de conhecimento que abrange uma variedade de
sistemas, aqui chamados de sistemas de recuperação de informação (``IRS -- Information Retrieval System''), que se manifestam de diferentes formas.
Por exemplo, IRSs são componentes centrais em motores de busca,
sistemas de filtragem de informação, sistemas de sumarização de documentos, sistemas de perguntas e respostas,
sistemas de recomendação, dentre outros. \cite{Ceri2013}

\section{Sistema de Recuperação de Informação}\label{sec:sobre-recuperacao-de-informacao}

Defino o IRS como a implementação de um modelo de recuperação de informação (``IRM -- Information Retrieval Model'').
Um IRM descreve o funcionamento da recuperação de informação em um sistema e é formalmente definido pela quádrupla: \cite{Ceri2013}

\begin{equation}
\text{IRM} = \{D, Q, F, R(q_k, d_j)\}
\label{eq:irm}
\end{equation}

onde
\begin{itemize}
    \item \(D\) é um conjunto de representações \(d_j\) de documentos;
    \item \(Q\) é um conjunto de representações \(q_k\) de consultas;
    \item \(F\) é a estratégia adotada para modelar a representação de documentos e consultas e suas relações;
    \item \(R(q_k, d_j)\) é a função de ranqueamento que associa \(d_j\) a um número real que denota sua relevância para \(q_k\).
\end{itemize}

Autores costumam categorizar modelos em diferentes abordagens, a depender de como são definidos os elementos $D$, $Q$, $F$ e $R$ do modelo.
Em essência, modelos são categorizados a partir de seu entendimento sobre relevância e da estratégia adotada para recuperar documentos relevantes.

Modelos que se encaixam na abordagem de correspondência exata compreendem relevância e adotam uma estratégia para recuperar documentos fundamentalmente diferente da estratégia adotada por modelos que se encaixam ou na abordagem de espaço vetorial, ou de modelos que se encaixam na abordagem probabilística.
\cite{Hiemstra2009}

% Importante notar que não há uma forma precisa, ou mesmo uma preocupação, de categorizar fielmente um modelo
% \footnote{Por exemplo, algumas abordagens probabilísticas de representar documentos, como BM-25, são também uma forma abordagem de espaço vetorial.}
% e que não existe uma abordagem ou um modelo melhor que outros para implementação de um sistema.
Não é incomum, entretanto, encontrar descrições de modelos que se encaixam em diferentes abordagens,
de modo que não existe um consenso ou uma clareza de classificação dos diferentes modelos.
\footnote{Alguns autores descrevem modelos baseados em comparação de palavras-chave como modelos de correspondência exata \cite{} e outros os descrevem como modelos de espaço vetorial \cite{Hiemstra2009}.}
A verdade é que um modelo pode carregar características de diferentes abordagens, sem que isso descaracterize sua abordagem principal.
Por exemplo, modelos de espaço vetorial vão usar correspondência exata para encontrar documentos relevantes e usar um modelo probabilístico para ranquear seus documentos.

Cumpre observar, apenas, que a definição de um IRM é importante para delinear e conduzir experimentos e constatar sua eficácia para uma aplicação específica.
A descição formal do modelo é necessária para garantir consistência e para ``garantir que o modelo pode ser implementado em um sistema real''.\cite{Hiemstra2009}

Ao mesmo tempo que um modelo é responsável por ser o esboço ou plano (\textit{blueprint}) de um IRS, um IRS também pode implementar diferentes modelos.
Como exemplo, o buscador do Google permite buscas por correspondências exatas de palavras-chave e booleanas
\footnote{https://support.google.com/websearch/answer/2466433}
e, ao mesmo tempo, implementa outros modelos que permite encontrar maior relevância em buscas cotidianas, como o PageRank ou modelos baseados em redes neurais.
\footnote{https://searchengineland.com/how-google-uses-artificial-intelligence-in-google-search-379746}

Ao fim, a implementação de um IRS e os modelos escolhidos para serem implementados, dependerão das características dos documentos, das especificidades das consultas e da estratégia empregada na representação, recuperação e ranqueamento de um documento na coleção.
Principalmente, a maneira que um IRM compreende relevância influencia diretamente na definição de seus elementos.

E se, como argumentado, a definição de relevância é específica de um problema, então a escolha do IRM implementado por um IRS será primordialmente pautada pelo resultado que se pretende alcançar, considerando as características reais dos documentos, consultas e usuários.
Ou seja, a validade e eficácia do IRM é julgada pelos resultados obtidos após sua tradução em IRS, avaliados em um contexto específico, definido um objetivo específico.

Portanto, antes da implementação de qualquer IRS, é necessário definir o problema que pretendo resolver nesse trabalho e as formas que uso para resolver esse problema.

\section{O Problema de Ranqueamento}\label{sec:o-problema-de-ranqueamento}

O problema de ranqueamento de documentos pode ser definido como, ``dada uma necessidade de informação expressada por meio da consulta $q$, a tarefa de [...] retornar uma lista
ranqueada de $k$ textos $\{d_1, d_2 ... d_k\}$ de uma coleção de textos $C = \{d_i\}$ arbitrariamente grande, mas finita, que maximiza uma métrica de interesse, por exemplo, nDCG, AP, etc''\cite{}.

Um IRM busca solucionar o problema de ranqueamento através de uma função de ranqueamento \(R(q_i, d_j)\) que associa \(d_j\) a um número real $s_j$ que denota sua relevância para \(q_i\).
A lista ranqueada de textos pode ser ``mais explicitamente caracterizada como $\{(d_1, s_1), (d_2, s_2) ... (d_k, s_k)\}$ com a restrição de que $s_1 > s_2 > ... > s_k$''. \cite{}

No contexto do meu objetivo, que é desenvolver um mecanismo de busca para literatura acadêmica brasileira, estou restringindo a questão ao ranqueamento de trabalhos acadêmicos. Aqui, os documentos se referem especificamente a títulos, resumos e palavras-chave, e as consultas são feitas através de um campo em uma página web.
Como solução deste problema, sugiro a implementação e posterior avaliação de eficácia de dois IRMs.

O primeiro, chamado aqui de \textbf{modelo de correspondência exata}, representa documentos e consultas por meio de vetores esparsos e utiliza funções probabilísticas para comparar esses vetores e realizar ranqueamento. O segundo, chamado aqui de \textbf{modelo de busca semântica}, representa documentos e consultas por meio de vetores densos e utiliza operações algébricas para ranqueamento.

Nesses dois IRMs, a função de ranqueamento busca estimar a relevância de um documento $d$ em relação a uma consulta $q$, por meio de uma função $\Delta$ que calcula a similaridade entre as representações de $q$ e $d$, calculadas por meio das funções $\phi_q$ e $\phi_d$:

\begin{equation}\label{eq:irm-rank}
    R(q, d) = \Delta(\phi_q(q), \phi_d(d))
\end{equation}

Entendo que ambos os modelos são variações da abordagem de modelos de espaços vetoriais, uma vez que
usam representações para documentos e consultas baseados em uma distribuição estatística dos termos na coleção de documentos.
Entretanto, ambos são fundamentalmente diferentes na forma que criam essas representações dos documentos e consultas e na forma que calculam o ranqueamento, ou seja, na forma que atribuem relevância.
Nas restante do capítulo justifico a estratégia usada por esses IRMs para representar documentos e consultas e, em seguida, explico suas diferenças na forma de abordar essa estratégia.
Na seção \ref{sec:representacao-vetorial-de-texto}, discuto como documentos e consultas são representados nesses IRMs.
Na seção \ref{sec:ranqueamento-de-documentos}, discuto como documentos são ranqueados nesses IRMs.
Na seção \ref{sec:arquitetura-de-um-sistema-de-recuperacao-de-informacao}, discuto os aspectos gerais de um IRS sob a ótica dos IRMs propostos.

\section{Representação Vetorial de Texto}\label{sec:representacao-vetorial-de-texto}

Semântica vetorial é o nome que se dá à forma padrão de representar texto por meio de vetores.
A base da atual semântica vetorial nasceu nos anos 50, quando foram propostas ideias de representar
o significado de uma palavra por meio de sua distribuição no uso da linguagem e de representar essa distribuição por meio de um vetor.
\cite{JurafskyMartin2023}

Essa abordagem é chamada de hipótese distribucional, que estipula que palavras que ocorrem em um mesmo contexto possuem significados similares. \cite{PilehvarCamacho-Collados2022}
Aqui, possuir ``significado similar'' significa que seu uso na linguagem é similar, que ela ocorre nas mesmas posicões em uma frase ou que mantém relação parecida com as demais palavras.
Palavras que ocorrem em um mesmo contexto, teriam, assim, similaridade semântica.
Talvez elas sejam palavras sinônimas, ou talvez elas carreguem algum sentido comum quando usadas em um contexto específico.
\footnote{Cumpre notar que algumas palavras que carregam similaridade semântica em um contexto podem simplesmente não ter similaridade em outro contexto.}
Sob a mesma premissa, documentos que possuem distribuição parecida de palavras são considerados similares.

A ideia por trás da semântica vetorial é representar palavras como um ponto em um espaço multidimensional derivado das distribuições de vizinhanças, de palavras e palavras que estão próximas a elas,
possibilitando a medida de um grau de similaridade de termos ou documentos por meio de um cálculo de similaridade entre suas representações vetoriais.\cite{JurafskyMartin2023}

As representacões vetoriais podem ser classificadas entre técnicas baseadas na contagem de frequência de ocorrência
ou co-ocorrência das palavras, que resultam em representações esparsas de texto usadas no modelo de correspondência exata, e técnicas baseadas em redes neurais, que resultam
em representações densas de texto usadas no modelo de busca semântica.

Explicada a estratégia da representação vetorial de texto, que baseia os IRMs baseados no modelo de espaço vetorial, esclareço as diferenças entre as estratégias usadas pelo modelo de correspondência exata \ref{sec:modelo-de-correspondencia-exata} e pelo modelo de busca semântica \ref{sec:modelo-de-busca-semantica}.

\section{Modelo de Correspondência Exata}\label{sec:modelo-de-correspondencia-exata}

Para explicar o modelo de correspondência exata, explico primeiramente a forma usada por esse modelo para representar o texto para, em seguida, explicar a forma de calcular a similaridade.

Representações esparsas de texto são geralmente baseadas em uma matriz de co-ocorrência, uma forma de representar a ocorrência de um
termo em relação a um documento.
Essas são chamadas de matrizes de termo-documento.
\footnote{Existem também as matrizes que relacionam termos a outros termos (matrizes de termo-termo), mas essas não possuem relevância para este trabalho.}
\cite{JurafskyMartin2023}

Nessas matrizes, um vetor coluna denota um documento, que é representado pelas frequências de seus termos.
Cada linha $i$ da matriz representa uma palavra e cada coluna $j$ representa um documento.
Ou seja, o valor $M_{i,j}$ representa a frequência da palavra $P$ da linha $i$ em um documento $D$ da coluna $j$.
Como exemplo, veja a coleção de documentos abaixo e parte de sua matriz de co-ocorrência termo-documento:

\begin{center}
\begin{tabularx}{\textwidth}{|c|X|}
\hline
Documento & Texto \\
\hline
Documento 1 & Esperou em um banco para pagar a conta. Depois de horas esperando, saiu do banco e foi ao parque. \\
\hline
Documento 2 & Sentou em um banco da praça para aguardar sua chegada. \\
\hline
Documento 3 & Eu banco a despesa. \\
\hline
\end{tabularx}
\end{center}


\begin{table}[ht]
\centering
\caption{Matriz Termo-Documento.}
\label{tab:term-doc-matrix}
\begin{tabular}{|l|c|c|c|}
\hline
Termo     & Documento 1 & Documento 2 & Documento 3 \\
\hline
a         & 1           & 0           & 1           \\
\hline
aguardar  & 0           & 1           & 0           \\
\hline
ao        & 1           & 0           & 0           \\
\hline
banco     & 2           & 1           & 1           \\
\hline
chegada   & 0           & 1           & 0           \\
\hline
\multicolumn{4}{c}{\vdots} \\
\end{tabular}
\end{table}


Em uma matriz $m \times n$, cada vetor de palavra tem dimensão $n$ e cada vetor de documento tem dimensão $m$.
Uma consequência desse tipo de representação é que o número de linhas é igual ao tamanho do vocabulário e o número de colunas é igual ao número de documentos.
Essa representação ocupa um espaço proporcional a $m \times n$ e é considerada esparsa para a maior parte dos documentos, já que a distribuição de palavras por documento é pequena quando comparada ao número de documentos.
\cite{JurafskyMartin2023}

% [COMENTÁRIO]
% Futuramente, cumpre ressaltar que usar essa representação para recuperação de informação
% exige pensar em armazenamento e recuperação de matrizes com essas
% características específicas.

IRMs que usam representações esparsas de texto calculam a similaridade entre documentos por meio de uma análise da frequência dos termos presentes na consulta e que também estão presentes nos documentos da coleção.
Isso pode ser feito pela análise dos valores de um vetor coluna da matriz acima.

Entretanto, uma análise crua da frequência de termos por documento pode não ser representativa da similaridade entre documentos.
Primeiro, a alta frequência de uma palavra em um documento pode afetar desproporcionalmente uma comparação entre documentos simplesmente por um documento ter mais palavras que o outro.
Segundo, algumas palavras aparecem com muita frequência em documentos e não
carregam grande significado semântico.
Necessita-se, portanto, de uma medida, baseada nos valores das frequências, que passe a representar
a importância de uma palavra para um documento.

Por esses motivos, a análise de frequência é feita a partir atribuição de pesos aos termos (\textit{weighting}), seguida de alguma medida de associação entre os termos da consulta e os termos do documento.
Assim, uma função de atribuição de valor (função de \textit{scoring}) pode ser descrita como: \cite{}

\begin{equation}
\label{eq:score}
    S(q,d) = \sum_{t \in q \cap d} w(t)
\end{equation}

onde $w$ é uma função que atribui um peso a um termo $t$, a partir de uma estatística associada ao termo no documento ou coleção.

As estatísticas associadas comumente usadas
\footnote{Essas são comumente usadas para computar variações da família de funções chamadas de frequência de termo e inverso da frequência por documento (tf-idf -- \textit{Term Frequency Inverse Document Frequency}).}
são:
(a) frequência do termo no documento (tf -- \textit{term frequency})
% (equação \ref{eq:tf})
, (b) frequência do termo na coleção de documentos ou seu inverso (df \textit{-- document frequency} e idf { -- \textit{inverse document frequency}})
% (equações \ref{eq:df} e \ref{eq:idf})
, e (c) tamanho do documento (dl \textit{-- document length})
% (equação \ref{eq:dl})
. \cite{}

\begin{equation}
\text{tf}(t, d) = count(t, d)
\label{eq:tf}
\end{equation}

\begin{equation}
\text{df}(t) = \left| \{ d \in D : t \in d \} \right|
\label{eq:df}
\end{equation}

\begin{equation}
\text{idf}(t) = \log \frac{|D|}{\text{df}(t)}
\label{eq:idf}
\end{equation}

\begin{equation}
\text{dl}(d) = \sum_{t \in d} f(t,d) \\
\label{eq:dl}
\end{equation}

\begin{equation}
\text{avgdl}(d) = \frac{1}{|D|} \sum_{d \in D} \text{dl}(d)
\label{eq:}
\end{equation}

% open source connections bm25
% staff.city.ac.uk
% Essa família de funções que busca amenizar os problemas mencionados acima ao calcular um peso para um termo $t$, por meio do produto $tf(t,d) \times idf(t)$, em que $tf(t,d)$ é a frequência normalizada do termo $t$ no documento $d$, e $idf(t)$ é o inverso da frequência do termo na coleção de documentos, ou seja, o número total de documentos dividido pelo número de documentos em que o termo ocorre:
% \begin{equation}
% \text{tf-idf}(t, d) = \frac{f_{t, d}}{\sum_{t' \in d} f_{t', d}} \times \log \frac{N}{\text{df}(t)}
% \label{eq:tfidf_expanded}
% \end{equation}

A função $w$ mais usada em sistemas de busca por correspondência exata é a função BM25 (\textit{Best Match 25}): \cite{}

\begin{equation}
\text{BM25}(d,q) = \sum_{t \in q \cap d} \frac{(k + 1) \times \text{tf}(t,d)}{k_1 \times ((1-b) + b \times \frac{\text{dl}(d)}{\text{avgdl}(d)}) + \text{tf}(t,d)} \times \log \frac{|D| - \text{df}(t) + 0.5}{\text{df}(t) + 0.5}
\end{equation}

em que o segundo termo da equação é idf adaptado e $k$ e $b$ são hiperparâmetros.

Variações da BM25 são o estado da arte na representação vetorial esparsas de documentos e, por esse motivo, essa função ainda é largamente implementada em sistemas de recuperação de informações. \cite{}

Uma vantagem desses IRMs é que eles são eficientes, já que o cálculo de relevância de uma consulta tem complexidade proporcional ao número de termos da consulta, e são simples de implementar, já que dependem
apenas da criação de um índice invertido.
\textbf{[TODO MENCIONAR LUCENE COMPLETAR COM INFORMAÇÕES DO CAPÍTULO KEYWORD SEARCH]}

No entanto, o cálculo de similaridade baseado em análise de frequências apresenta desvantagens significativas. A matriz exemplificada na Tabela \ref{tab:term-doc-matrix} ilustra claramente as limitações dessa abordagem.

Primeiro, uma representação de um texto aqui descrita não consegue capturar a polissemia de um termo.
No caso exemplo, o termo ``banco'' apresenta 3 significados diferentes e uma consulta ``bancos de parque''
provavelmente teria como retorno mais relevante o Documento 1 (independentemente ta medida de associação a ser considerada),
ao invés do Documento 2, que é factualmente mais relevante para a consulta.

Segundo, e mais importante, esse modelo sofre com a incompatibilidade de vocabulário (\textit{vocabulary mismatch}).
Esse é um problema conhecido em PNL e se manifesta quando ``pessoas diferentes nomeiam a mesma coisa ou conceito de formas diferentes''
\footnote{https://en.wikipedia.org/wiki/Vocabulary_mismatch}
.
No caso exemplo, a consulta ``pagar os custos'' provavelmente teria como documento mais relevante o Documento 1, o que não é necessariamente verdade,
dado que o Documento 3 é ``Eu banco a despesa'', que contém uma ideia semanticamente mais parecida com a consulta.

Vetores esparsos têm uma boa capacidade de representar documentos para uma comparação por similaridade textual, mas não para similaridade semântica.
Como se pode verificar pela explicação acima, para o caso em que o documento relevante define a uma mesma ideia com diferentes palavras, ou usa palavras sinônimas, mas não uma distribuição de palavras similar à da consulta, uma busca que usa BM25 torna-se insatisfatória. \cite{thakur-2021-BEIR}

Portanto, apesar do IRM descrito depende da correspondência exata entre os termos da consulta e os termos do texto e acaba enfrentando o desafios de linguagem sem solução simples.
\footnote{A necessidade de correspondência exata é amenizada por meio de algumas técnicas, como \textit{stemming} e \textit{lemmatization}, mas
ainda assim, essas técnicas não são capazes de superar o problema da incompatibilidade de vocabulário.}
Historicamente, três abordagens têm sido usadas para superar esse desafio:
\begin{itemize}
    \item[(a)] melhorar as representações da consulta;
    \item[(b)] melhorar as representações dos documentos; e
    \item[(c)] usar representações não dependentes de correspondência exata.
\end{itemize}
\cite{}


Uma abordagem que deriva, de certa forma, da abordagem (c), é a utilização de representações textuais densas para calcular a similaridade semântica. \cite{}
De fato, para solucionar os problemas com o atual IRM, necessita-se uma nova forma de representar documentos e consultas, uma que considere o significado semântico dos termos, com menor ênfase na contagem de termos léxicos e maior ênfase em adquirir intuição sobre seu significado no contexto. \cite{bhaskar-craswell-2018}

% [INFORMAÇÃO REMOVIDA]
% Em matrizes termo-termo, termos são relacionados com outros termos que ocorrem no mesmo contexto.
% Nesse caso, cada palavra P ocupa uma linha i e uma coluna j, ou seja, cada vetor de palavra tem dimensão |P|,
% e o valor $Mi,j$ é a frequência em que uma palavra ocorre no mesmo contexto de outra.
% Um contexto é definido como uma janela de $k$ palavras na mesma sentença e a ocorrência no mesmo contexto é medida como
% $(k-1)/2$ palavras à direita e $(k-1)/2$ palavras à esquerda.
% Essa representação ocupa um espaço proporcional a |P| x |P| e também é esparsa.
% \cite{JurafskyMartin2023}

\section{Modelo de Busca Semântica}\label{sec:modelo-de-busca-semantica}

A solução proposta para o problema observado em abordagens puramente léxicas foi a utilização de redes neurais para criar representações densas de texto (\textit{embeddings}).
Nessas representações, o texto é mapeado para um espaço vetorial de baixa dimensão, em que todas as dimensões têm valores diferentes de 0.
Essas representações são chamadas de densas em oposição às representações esparsas observadas no modelo de correspondência exata, em que a dimensão era proporcional ao tamanho do vocabulário.

O uso de redes neurais para resolver o problema do ranqueamento têm se baseado em duas abordagens, modelos que se valem do rerranqueamento de documentos retornados por um modelo de correspondência exata e modelos que usam técnicas de recuperação profunda (\textit{dense retrieval}), que computam o ranqueamento diretamente nas representações vetoriais capazes de capturar significado semântico. \cite{}

A primeira abordagem evidentemente sofre com o problema de \textit{vocabulary mismatch} em sua primeira etapa de recuperação de informação. Para além disso, como notam os autores \citeauthor{}, existem outros motivos para preferir a segunda abordagem. Primeiro, o custo computacional de rerranquear documentos é caro e é feito durante a consulta, ao invés de ser feito na etapa de pre-processamento como no modelo de \textit{dense retrieval}, que calcula apenas a inferência da consulta e usa uma função de similaridade rápida de calcular em uma coleção de vetores densos usando soluções baseadas em \textit{nearest neighbor search}. Segundo, arquiteturas de várias camadas são pouco elegantes e trazem uma série de problemas para o treinamento do modelo de rede neural que se localiza na segunda camada.

\textbf{[COMPLETAR COM A TÉCNICA PRE-BERT / PÓS-BERT / FORMA QUE FUNCIONA DENSE RETRIEVAL MODELS}]

Portanto, \textit{dense retrieval} soluciona o problema de ranqueamento através da geração de representações densas dos documentos e do cálculo uma métrica simples de similaridade entre a representação da consulta e do documento.

Essa métrica simples é geralmente obtida através de operações de álgebra linear que medem similaridade entre vetores.

Um método algébrico comum de medir a similaridade entre dois vetores é através do produto interno, definido por:

\begin{equation}
\text{dot}(\mathbf{u}, \mathbf{v})  = \mathbf{u} \cdot \mathbf{v} = \sum_{i}u_{i}v_{i}
\label{eq:dot_product}
\end{equation}

Uma interpretação para o resultado do produto interno é a medição de quanto um vetor $u$ está no mesmo sentido e direção de outro vetor $v$. Essa é uma interpretação conveniente para compreender a medida de similaridade entre duas representações vetoriais de texto.

Um problema de realizar medições com o produto interno é comparar similaridades, pois $dot(u,v)$ pode ser arbitrariamente grande ou pequeno a depender dos valores dos componentes $u_i$ e $v_i$ dos vetores. Nesse caso, pode parecer mais sensato calcular o produto interno com vetores normalizados, o que resulta no valor do cosseno do ângulo entre os vetores e leva todas as comparações
a um intervalo entre 0 e 1, uma vez $v_i, u_i \geq 0, \forall i$:

\begin{equation}
\cos (\mathbf{u}, \mathbf{v})  = \frac{\mathbf{u} \cdot \mathbf{v}}{\|\mathbf{u}\| \|\mathbf{v}\|}
\label{eq:cosine_similarity}
\end{equation}

Assim, é possível comparar similaridades entre diferentes vetores de palavras.
Quanto mais próximo de 1 for o resultado, mais similares são os termos comparados.
Essa técnica funciona para qualquer modelo vetorial de texto.

No caso da similaridade entre um documento $d$ e um conjunto de documentos $V$, tal que $V = \{\mathbf{v_1}, \mathbf{v_2}, ... \mathbf{v_n}\}$,
pode-se calcular o cosseno entre a representação de $d$, denotada por $\mathbf{u}$, e o centróide $\mathbf{c}$ de $V$, dado por:

\begin{equation}
\mathbf{c} = \frac{1}{n} \sum_{i=1}^{n} \mathbf{v_i}
\label{eq:centroid}
\end{equation}

Dessa forma, a similaridade entre $d$ e $V$ é dada por $\cos⁡ (\mathbf{u},\mathbf{c})$ .

A escolha da forma de comparação, entretanto, vai depender da arquitetura do modelo e de seu treinamento. Alguns modelos são treinados para serem usados com produto interno, outros com cálculo de cosseno e, para outros, não vai fazer qualquer diferença.

Desde a publicação do texto de referência usado para a elaboração dessa seção \cite{}, diversas soluções de software que cuidam do \textit{pipeline} de \textit{dense retrieval}, popularmente chamadas bases de dados vetoriais, surgiram.
que se baseiam na geração de representações densas de texto seguida da comparação de similaridade entre a representação densa de uma consulta e dos textos.
% https://www.youtube.com/watch?v=Mt7UJNKxscA&list=PLSg1mducmHTPZPDoal4m59pPxxsceXF-y&index=7



Modelos pré-treinados só passaram a ser usados em problemas de busca e ranqueamento de texto com modelos baseados na arquitetura Transformers \cite{}
\footnote{Como descrevem os autores \citeauthor{}, a partir de Outubro de 2019 alguns artigos de blog do Google e Microsoft reportam a utilização de modelos de linguagem para melhorar a busca em seus serviços.}
\cite{DBLP:journals/corr/abs-1810-04805}
Uma grande vantagens dessas novas arquiteturas é sua generalização para diferentes tarefas de IR e para diferentes conjuntos de dados. \cite{}
Com isso, é possível usar modelos pré-treinados para alcançar bons resultados em conjuntos de dados diferentes daqueles para os quais os modelos foram treinados. \cite{}

Desses modelos, BERT \cite{} é o mais representativo desta leva de modelos de linguagem.
Como colocado pelos autores \citeauthor{}, BERT se destacou por ``reunir muitos ingredientes cruciais para produzir enormes saltos na eficácia de uma ampla gama de tarefas de processamento de linguagem natural''.
Em um movimento inspirado por BERT, vários outros modelos de linguagem surgiram a partir de 2019, expandindo o estado da arte em tarefas de geração de representações densas de texto.

Tanto pesquisadores \cite{}, quanto a indústria \cite{}, têm cada vez mais usado comparações entre representações densas de texto para ranquear documentos a partir de uma consulta.
Ao fim, aprender representações semânticas de um texto é importante para lidar com os problemas de incompatibilidade de vocabulário ou polissemia, mas usar comparações por termos também é importante para lidar com termos raros ou intenções (\textit{intents}). \cite{bhaskar-craswell-2018}
De acordo com \citeauthor{bhaskar-craswell-2018}, ``na prática um modelo de IR deve sopesar a correspondência exata e inexata para um termo consultado''. Para esses autores, a decisão de que forma de correspondência usar para uma consulta deve considerar o contexto.

% Diversos modelos foram disponibilizados pela biblioteca Sentence Transformers \footnote{}, implementada em Python.
% Desde então, vários outros modelos pré-treinados são disponibilizados na plataforma Hugging Face \footnote{},
% apropriados para diferentes tarefas de IR.
% Nesse trabalho, alguns modelos pré-treinados serão utilizados como base para criação dos embeddings.

% - Diferentes associações ou similaridades
% - Analogia / Similaridade Relacional



% ----------------------------------------------------------------------------------------------------------------------
% TÓPICOS:
%- Word2vec: pequenos e densos; skip-gram; SGNS; static embeddings;
% - O problema com Word2Vec e Contextual Embeddings
%- Dynamic Contextual Embeddings
% ----------------------------------------------------------------------------------------------------------------------

% [2]

% Quanto maior a dimensionalidade do embedding maior a qualidade dele.

% OpenAI and Cohere embeddings, which require a paid API call to generate them, can be considered higher quality due to a dimensionality of a few thousand.

% One reason it makes sense to use a paid API to generate embeddings is if your data is multilingual (Cohere is known to possess high-quality multilingual embedding models that **[are known to perform better](https://docs.cohere.com/docs/multilingual-language-models#model-performance)** than open source variants).

% E se, ao invés de criar mais embeddings, eu mudar a dimensionalidade do embedding?

% In his excellent review post**[3](https://thedataquarry.com/posts/vector-db-2/#fn:3)**, Colin Harman describes how a lot of companies, due to the plethora of vector DB marketing material out there today, experience “tunnel vision” when it comes to the search & retrieval landscape. As practitioners, we have to remember that vector databases are not the panacea of search – they are very good at *semantic* search, but in many cases, traditional keyword search can yield more relevant results and increased user satisfaction**[4](https://thedataquarry.com/posts/vector-db-2/#fn:4)**. Why is that? It’s largely to do with the fact that ranking based on metrics like cosine similarity causes results that have a higher similarity score to appear above partial matches that may contain specific input keywords, reducing their relevance to the end user.

% *Cross Encoders Models →* usar depois dos Bi-directional?

% including an agent-based framework like LangChain

% - o que melhora um embedding? Um embedding parece melhorar quando eu divido mais um grande texto. Mas o embedding também piora quando eu diminuo até um nível atômico, então existe um ponto ótimo. Qual o ponto ótimo? Eu acredito que sejam níveis que contêm um núcleo semântico. Então keywords deveriam ser divididas por letras maíusculas em um PDF, a não ser que elas sejam uma sigla. Ainda, nem todas as keywords seguem esse padrão, mas acho que só de pegar alguns já é uma vantagem. As frases podem ser divididas entre ponto. E o título é um ponto só.
% - Os embeddings ignoram stop words? Segundo esse link sim: https://github.com/UKPLab/sentence-transformers/issues/383. Eu consegui replicar isso:
% Outras Referências
% [2]https://thedataquarry.com/posts/vector-db-2/#how-are-embeddings-generated
% [3]https://techcrunch.com/sponsor/nvidia/how-the-revolution-of-natural-language-processing-is-changing-the-way-companies-understand-text/
% [4] https://qdrant.tech/articles/hybrid-search/
% [5] https://colinharman.substack.com/p/beware-tunnel-vision-in-ai-retrieval
% [6] https://thedataquarry.com/posts/vector-db-2/#fn:4

% https://www.sbert.net/examples/domain_adaptation/README.html?highlight=fine%20tuning

% Information retrieval is the process of searching and returning relevant documents for a query from
% a collection. In our paper, we focus on text retrieval and use document as a cover term for text of
% any length in the given collection and query for the user input, which can be of any length as well.
% Traditionally, lexical approaches like TF-IDF and BM25 [ 55 ] have dominated textual information
% retrieval. Recently, there is a strong interest in using neural networks to improve or replace these
% lexical approaches. In this section, we highlight a few neural-based approaches and we refer the
% reader to Lin et al. [37] for a recent survey in neural retrieval
% https://arxiv.org/pdf/2104.08663.pdf

%[COMENTÁRIO: mas isso nao considera o tamanho do documento... Eu entendo que squash com logaritmo acaba normalizando tambem,
%mas a ideia deveria ser normalizar dentro de um documento, pegar a frequencia dentro do documento.]
%pequenas diferenças de frequência de uma palavra entre documentos não deveria representar grandes diferenças
%em similaridade.
%É mais interessante, assim, modificar a escala para graduar uma palavra por sua frequência do documento apenas quando ela
%for extremamente frequente.

% No caso de termos, um motivo.
% Palavras muito frequentes em todos os contextos representam alto ruído (assim como no caso de documentos), mas não carregam
% grande significado semântico.
% Nesse caso, a medida usada para resolver esse problema entre termos é \textit{Pointwise Mutual Information} (PMI), que objetiva
% calcular a probabilidade em que duas palavras aparecem em um contexto em relação à probabilidade que seria esperado encontrar essas palavras em um
% mesmo contexto, dado que elas são independentes.
% Isso é representado por um cálculo de probabilidades, em que p(x,y) é a probabilidade conjunta de aparecerem os termos x e y
% em um mesmo contexto e p(x) e p(y) são as probabilidades marginais de x e y ocorrerem, respectivamente.

% Sabendo que, para t e w termos de uma coleção, $p(t) = \frac{count(t)}{\sum{w}count(w)}$,

% $$\text{PMI}(x, y) = \log \frac{p(x, y)}{p(x) \cdot p(y)}$$

% Como a relação de co-ocorrência entre termos é simétrica, pode-se interpretar as dimensões de um vetor linha da matriz de co-ocorrência
% termo-termo como sendo contextos, o que faz dessa uma matriz de co-ocorrência termo-contexto.
% Assim, para um termo t e um contexto c,

% $$\text{PMI}(t,c) = \log \frac{p(t,c)}{p(t) \cdot p(c)}$$

% Como o valor de PMI pode ser negativo, algo a que não se consegue atribuir um sentido no caso de similaridade entre termo
% e contexto, usa-se a medida Positive PMI (PPMI), calculada como

% $$\text{PPMI}(x, y) = \max \left(0, \log \frac{p(x, y)}{p(x) \cdot p(y)}\right)$$

% Essa medida tem a tendência de ser enviesada para contextos pouco frequentes (imagine p(c) << 1).
% Uma forma de corrigir isso é computar a probabilidade por meio da função $P_{\alpha}(c)$, em que se eleva a probabilidade
% de um contexto à $\alpha$ potência, com o objetivo de
% ``aumentar a probabilidade de contextos raros e diminuir o PMI ($P_{\alpha} > P(c)$ quando $c$ é raro)''
% \cite{JurafskyMartin2023} p. 118.


\section{Avaliando a Qualidade de um IRM: Métricas de Ranqueamento}\label{sec:metricas-de-ranqueamento}

A relevância é o principal critério para avaliar a qualidade do ranqueamento.
Sua natureza subjetiva e dinâmica requer avaliação dentro do contexto específico do sistema de recuperação de informação,
com base em métricas predefinidas.
Portanto, necessita-se saber aprioristicamente a relevância de um documento para uma consulta.
Isso é feito por meio de julgamentos de relevância.

Julgamentos de relevância (\textit{relevance judgments}) são avaliações humanas da relevância de uma coleção de texto para uma consulta.
Julgamentos de relevância servem a dois propósitos, podem ser usados para treinar um modelo de ranqueamento,
ou podem ser usados para avaliar funções de ranqueamento. \cite{}

Julgamentos de relevância são também chamados de qrels e representados por um conjunto de triplas $(q, d, r)$, onde $q$ é uma consulta, $d$ é um documento e $rel(q, d) = r$
é um julgamento de relevância do documento $d$ para uma consulta $q$.
Em geral, $rel(q, d) = r \in \{0, 1, 2, 3, 4\}$, onde $r = 0$ significa que $d$ não possui qualquer relevância para $q$ e $r = 4$ significa que
$d$ é muito relevante para $q$.

Em um problema de ranqueamento, a ordem é relevante porque se presume que o usuário não consumirá muito mais do que
10 ou 20 resultados para sua busca.
Esse argumento é apoiado por diversas pesquisas que afirmam que o usuário de um mecanismo de busca de páginas web dificilmente vai além da 2 página. \cite{}

É comum, portanto, apresentar uma métrica com uma lista $R$ cortada de 10 ou 20 resultados mais relevantes retornados pela consulta.
Quando isso acontece, é comum representar com a notação Métrica@$k$, onde $k$ é o tamanho da lista $R$ truncada. \cite{}

Quando a ordem dos documentos recuperados é irrerelevante,
ou seja, quando a relevância de um documento é tratada como binária,
métricas usadas na avaliação da relevância da
informação são precisão e recall, ou variações estatísticas dessas.
Precisão avalia a ``solidez'' do sistema, provendo uma medida para a proporção de documentos relevantes dentre os
recuperados. Precisão é definida por:

\begin{equation}
    \text{Precisão}(R,q) = \frac{\sum_{(i,d)\in R} \text{rel}(q,d)}{|R|}
\end{equation}

Recall avalia a ``completude'' do sistema, provendo uma medida para a proporção de documentos relevantes que não
foram recuperados. Recall é definida por:

\begin{equation}
    \text{Recall}(R,q) = \frac{\sum_{(i,d)\in R} \text{rel}(q,d)}{\sum_{d\in C} \text{rel}(q,d)}
\end{equation}

% Sejam \textit{true positives} (TP) os documentos relevantes e recuperados,
% \textit{false positives} (FP) os que não são relevantes e foram recuperados,
% \textit{true negatives} (TN) os que não são relevantes e não foram recuperados,
% \textit{false negatives} (FN) os documentos que são relevantes e não foram recuperados,
% então precisão e recall podem ser descritos como
% $$P = \frac{TP}{TP + FP}$$
% $$R = \frac{TP}{TP + FN}$$
% Um sistema ideal deveria ter alta precisão e alto recall, próximos de 1, minimizando o número de FP e FN.
% Entretanto, isso nem sempre é possível, já que geralmente existe um \textit{trade-off} entre precisão e recall. [5 p.8]
% As métricas apresentadas acima não levam em consideração a ordem dos documentos retornados, e funcionam bem para avaliar sistemas que retornam conjuntos de documentos em qualquer ordem.
% Nem todos os sistemas de recuperação de informação se inserem nesse contexto.
% Para alguns sistemas, a ordem, ou \textit{rank}, é uma qualidade importante para a recuperação.
% Nesses casos, outras métricas devem ser consideradas, como o ganho de precisão ao aumentar a recall, \textit{Average Precision} (AP), ou a precisão em relação aos primeiros k documentos, \textit{Precision at k} (P@k).
% É isso que a precisão média calcula
% $$\text{AP} = \frac{1}{\text{R}} \sum_{k=1}^n P(k) \times \text{rel}(k)$$
% A precisão média calcula a precisão ao longo de diferentes k-cortes do resultado, somando e obtendo a média das precisões obtidas.

É possível, ainda, balancear ambas as métricas com uma média harmônica, métrica chamada de \textit{F-measure} ou com Precisão Média (\textit{AP -- Average Precision}).

Para o problema de ranqueamento, a métrica mais relevante é o Ganho Acumulado com Desconto Normalizado (\textit{nDCG -- Normalized Discounted Cumulative Gain)}, métrica ``especificamente desenvolvida para julgamentos de relevância graduados''.\cite{}

O Ganho Acumulado com Desconto (\textit{DCG -- Discounted Cumulative Gain}) para uma lista ranqueada $R = {(d_i, d_i)}$ é definido como:

\begin{equation}
    \text{DCG}(R,q) = \sum_{(i,d)\in R}\frac{2^{rel(q,d)} - 1}{\log_{2}(i+1)}
\end{equation}

A DCG é capaz de medir um ganho baseado na posição do documento recuperado por uma consulta, premiando quando um documento relevante aparece no topo da busca e penalizando quando aparece no fim da busca.
Isso porque, quanto maior $i$, maior o desconto sobre o numerador que computa a relevância.

O nDCG é definido por:

\begin{equation}
    \text{nDCG}(R,q) = \frac{\text{DCG}(R,q)}{\text{IDCG}(R,q)}
\end{equation}

onde IDCG representa o valor DCG para $R$ ideal para uma consulta $q$.
Assim, nDCG é um valor entre [0, 1], uma vez que DCG $\leq$ nDCG.

Essas métricas estão eficientemente implementadas na biblioteca trec\_eval
\footnote{https://github.com/usnistgov/trec\_eval}
, biblioteca bastante usada por
pesquisadores da área de recuperação de informação.

Algumas considerações devem ser feitas sobre sistemas especificos para recuperacao de literatura academica.


\section{Arquitetura de um Sistema de Recuperação de Informação}
\label{sec:arquitetura-de-um-sistema-de-recuperacao-de-informacao}

Até esta seção foram discutidos os IRMs como esboços de um IRS.
Nesta seção serão explicados detalhes da implementação de cada um dos IRMs descritos.
Acima foram tangenciadas questões de eficiência sob a ótica dos IRMs descritos, mas nada sobre a ótica da implementação dos IRMs.
Nesta seção, aproveito para descrever, em alto nível, como deve funcionar um IRS e como um IRS poderia implementar os IRMs descritos.
Aproveito para introduzir reflexões sobre robustês, sensibilidade e eficiência na implementação de um IRS.

Como discutido anteriormente, diferentes IRMs atendem a propósitos distintos. Eles variam conforme as características dos documentos, as especificidades das consultas feitas pelos usuários e a estratégia adotada para recuperar e ranquear documentos na coleção.

Analogamente, a arquitetura de um IRS é moldada pelos IRMs que servem como fundamentação teórica, bem como pelos aspectos práticos considerados no desenvolvimento de software.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Ainda assim, é possível identificar ``elementos
universais'' nas arquiteturas desses sistemas.
Tais ``elementos universais'' são discutidos a seguir, denominados ``componentes'' do sistema, conforme demonstrado na figura \ref{fig:irs-arquitetura},
em que as setas representam o fluxo de dados.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{figuras/arq-bas-1.png}
    \caption{Arquitetura Básica de um Sistema de Recuperação de Informação}
    \label{fig:irs-arquitetura}
\end{figure}

A ingestão e o processamento de dados é o primeiro passo no processo de construção de um IRS e é realizada pelo Componente de Ingestão dos Dados.
Isso envolve a coleta de dados de várias fontes, como a \textit{web}, bases de dados, sistemas de arquivos, entre outros.
Os \textit{crawlers} são geralmente utilizados para rastrear e baixar o conteúdo da \textit{web}.
Uma vez coletados, os dados são processados e são extraídas informações relevantes e removidas de informações irrelevantes por meio do uso de técnicas de PLN.

O processo de organização dos dados de forma que possibilite a busca eficiente é chamado de indexação,
realizada pelo Componente de Indexação.
Nesse componente, os dados são armazenados em uma estrutura chamada índice, que permite ao sistema recuperar
rapidamente os documentos relevantes.

É no Componente de Recuperação onde a consulta é executada para recuperar do índice.
É aqui onde são implementados algoritmos para determinar os resultados mais relevantes para a consulta.
Os resultados são, depois de recuperados, ranqueados e retornados ao usuário.

A interface do usuário, onde as consultas são inseridas, faz parte do Componente de Consulta.
Este sistema recebe as consultas dos usuários e as traduz em comandos que podem ser executados pelo sistema de busca.
O mecanismo pode aplicar algoritmos e heurísticas para processar a consulta e melhorar seus resultados.

Para o projeto proposto nesse estudo, os componente mais relevantes para a discussão dos IRMs propostos são os Componentes de Indexação, responsável criação e armazenamento das representações de documentos, e de Recuperação, responsável pela forma que a relevância dos documentos é computada. Ambos são responsáveis pela velocidade com que consulta é efetuada.

\subsection{Arquitetura Tradicional de um Sistema de Recuperação de Informação}\label{}

\subsection{Arquitetura de um Sistema de Recuperação Profunda}\label{}

% Ler: https://arxiv.org/pdf/2104.08663.pdf
% https://arxiv.org/pdf/2206.12993.pdf

% Ler: https://medium.com/@readsumant/understanding-ndcg-as-a-metric-for-your-recomendation-system-5cd012fb3397
%      https://www.youtube.com/watch?v=EnlLr6S7c5A
%      https://towardsdatascience.com/demystifying-ndcg-bee3be58cfe0
%      https://mariofilho.com/ndcg-normalized-discounted-cumulative-gain-em-machine-learning/


\chapter{Implementação do Sistema de Recuperação de Informação de Teses}
\label{ch:implementacao-do-sistema-de-recuperacao-de-informacao-de-teses}

O problema inerente aos algoritmos clássicos de recuperação de informação que se baseiam no tf-idf ou no BM-25 é que a
pesquisa deve ser conduzida utilizando as palavras exatas presentes no texto que se busca.
Essa abordagem é problemática, pois o usuário do sistema pode compreender o que deseja buscar sem, necessariamente,
conhecer as palavras específicas que constam no documento alvo.

Conforme discutido em \citeauthor{JurafskyMartin2023} p.277, uma alternativa promissora a esse dilema é a utilização de vetores densos (embeddings),
em contraste com os vetores esparsos tradicionais.
Essa estratégia tem sido explorada tanto por acadêmicos quanto por empresas em diversos artigos publicados.
Ao empregar modelos modernos como o BERT para codificar a consulta e os documentos, e ao realizar um produto vetorial
subsequente para calcular um score de similaridade, é possível oferecer resultados mais afinados com a intenção do usuário.

Entretanto, como destacado em \citeauthor{JurafskyMartin2023} p.278, a implementação de embeddings na busca ainda representa um desafio em aberto.
As áreas que despertam maior interesse incluem o ajuste preciso dos modelos para aumentar a relevância dos documentos
recuperados e a questão de como armazenar e recuperar vetores densos de maneira eficaz.

A discussão sobre este novo paradigma de busca semântica, que emprega embeddings,
oscila principalmente entre duas características fundamentais: qualidade e eficiência.
Estas são, frequentemente, qualidades conflitantes. Para alcançar uma qualidade superior,
o sistema requer uma busca mais abrangente, o que tende a aumentar a complexidade e reduzir a eficiência.
Por outro lado, para garantir eficiência, a busca de vetores densos precisa ser realizada de maneira aproximada,
o que pode comprometer a qualidade.

O desafio de arquitetar sistemas de busca pode ser correlacionado ao de arquitetar sistemas de recomendação.
Conforme \citeauthor{yan2021system}, esses sistemas são categorizados como ``discovery system: recommendation and search''.
Tal categorização ocorre pois os componentes de um sistema que realiza buscas são semelhantes aos de um sistema de
recomendação.
Na indústria, esses sistemas são geralmente divididos em dois componentes: a recuperação de candidatos e a etapa de
ranqueamento.
Essa divisão, embora não sempre implementada, é fundamental para compreendermos as principais partes dos sistemas modernos.
Adicionalmente, outras divisões são feitas, como a separação entre processos e componentes em ambientes offline e online.

O autor \citeauthor{yan2021system} apresentou exemplos de arquiteturas para sistemas de recomendação e de busca
utilizadas na indústria,
dividindo-as conforme os padrões: (a) Obter embeddings e construir um índice ANN ou um grafo de conhecimento para
encontrar itens similares; (b) Ranquear esses embeddings selecionados em um espaço menor, de acordo com outro modelo
ou heurísticas do negócio (por exemplo, o ranqueamento pode considerar características do usuário e histórico de busca
em uma \textit{feature store} criada offline).

Portanto, esses sistemas transformam a busca em embedding antes de aplicar o índice ANN para encontrar itens similares.
Também é possível utilizar grafos ou árvores de decisão, e há uma etapa de ranqueamento após a redução do espaço de
busca, usando características que não são consideradas no modelo de embedding. Modelos de ranqueamento também podem
ser usados, como um modelo de "learning to rank" ou classificação.

Outros autores expandem a noção da arquitetura desse sistema de 2 estágios para 4 estágios: Retrieval, Filtering,
Scoring e Ordering [2].
Com isso, além dos modelos de recuperação e ranqueamento, são adicionadas políticas
específicas para filtrar e ordenar o resultado para cada usuário. Isso permite melhorar o sistema de busca,
sem depender exclusivamente do modelo de recuperação ou ranqueamento.

Alguns exemplos desse tipo de sistema incluem o sistema de recomendação do Instagram e a linguagem IGQL, e a
arquitetura de sistema do Quora, que segue a divisão de Retrieval, Filtering, Scoring e Ordering. A Instacart
compartilhou uma arquitetura similar em 2016.

% Para compreender melhor como esses sistemas funcionam, estudei [3], que explica, em alto nível, como funciona a arquitetura de busca do Semantic Scholar. Basicamente, a query passa para a Elasticsearch (AWS), e os 1000 primeiros resultados são ranqueados pelo ranker, usando um ranker LightGBM com um objetivo LambdaRank.

% Na minha arquitetura, devo considerar o que é importante em cada etapa. Quero desenvolver um sistema que busque rapidamente teses, sem necessidade de criar novos embeddings ou índices rapidamente, então posso priorizar a velocidade de consulta. Algumas bases de dados podem ser otimizadas em diferentes níveis, dependendo do momento do sistema de busca.

% Em uma visão geral, destacam-se três componentes de um sistema de recuperação de informação: um módulo de obtenção da consulta, um módulo de análise da consulta e busca, e um módulo de administração do conteúdo. Estes módulos são uma visão geral e podem conter subdivisões, como um sistema de ingestão de dados, um parseador, etc.

% [1] Yan, Ziyou. (Jun 2021). System Design for Recommendations and Search. [eugeneyan.com](http://eugeneyan.com/). https://eugeneyan.com/writing/system-design-for-discovery/.
% ```latex
% @article{yan2021system,
%   title   = {System Design for Recommendations and Search},
%   author  = {Yan, Ziyou},
%   journal = {eugeneyan.com},
%   year    = {2021},
%   month   = {Jun},
%   url     = {https://eugeneyan.com/writing/system-design-for-discovery/}
% }
% ```
% [2] Recommender Systems, Not Just Recommender Models https://medium.com/nvidia-merlin/recommender-systems-not-just-recommender-models-485c161c755e
% [3] https://blog.allenai.org/building-a-better-search-engine-for-semantic-scholar-ea23a0b661e7

\section{Arquitetura do Sistema}\label{sec:arquitetura-do-sistema}

\section{Ingestão de Dados}\label{sec:ingestao-de-dados}
% ---------------------------------------------------------------------------------------------------------------------
% TÓPICOS:
% - Falar sobre o Crawler e sobre o repositório de teses da usp
% ---------------------------------------------------------------------------------------------------------------------

\section{Processamento de Conteúdo}\label{sec:processamento-de-conteudo}
% ---------------------------------------------------------------------------------------------------------------------
% TÓPICOS:
% - Document parsing -> reconhecimento e estruturação;
% - Análise léxica
% - Remoção de stop words????
% (Não mais que uma página)
% ---------------------------------------------------------------------------------------------------------------------

\section{Indexação}\label{sec:indexacao}
% ---------------------------------------------------------------------------------------------------------------------
% NOTAS:
% \cite{Hiemstra2009} discute, em p. 7, dificuldades da implementação do modelo de espaço vetorial. Isso porque o cálculo
% da similaridade de cosseno depende de todos os componentes da representação vetorial. Entretanto, isso não está disponível
% em um índice invertido. Na prática, os valores normalizados e o produto vetorial precisam ser usados. Então, ou eles são
% adicionados em um
%
% No podcast [...], o entrevistado discute: In memory index vs disk index → HMSW INDEX (good trade off but memory
% hungry) — memmap in qdrant (good inn performance) knn, qdrant is the best in disk. Hybrid search. Pre filtering vs
% post filtering. Self hosted vs cloud.
% ---------------------------------------------------------------------------------------------------------------------

% ---------------------------------------------------------------------------------------------------------------------
% TÓPICOS:
% Por que usar um índice.
% Quais são os tipos de índices usados.
% Quais os índices usados no meu sistema.
% - ANN (approximate nearest neighbor): an algorithm that uses distance algorithms to locate nearby vectors.
% - kNN: an algorithm that uses proximity to make predictions about grouping.
% - (SPTAG) Space partition tree and graph: a library for large scale approximate nearest neighbors.
% - Faiss: Facebook’s similarity search algorithm.
% - HNSW (hierarchical navigable small world): a multilayered graph approach for determining similarity.
% ---------------------------------------------------------------------------------------------------------------------

\section{Busca}\label{sec:busca}

\section{Ranqueamento}\label{sec:ranqueamento}

\section{Experimentos Para Avaliação do Sistema de Recuperação de Informação de Teses}\label{sec:experimentos-para-avaliacao-do-sistema-de-recuperacao-de-informacao-de-teses}

\chapter{Experimentos}
\label{ch:exper}
% ---------------------------------------------------------------------------------------------------------------------
% NOTAS
% -----
% Positioning the Query in the Vector Space
% Uma coisa para manter em mente é uma forma de adaptação de indexação por feedback descrita
% em \cite{Hiemstra2009} p.7. Um algoritmo de relevance feedback sugerido por Rocchio, em que um vetor é adaptado de
% acordo com o resultado da busca dos documentos e do resultado de relevância calculado .
% ---------------------------------------------------------------------------------------------------------------------

Neste capítulo, descrevo o processo de desenvolvimento e adaptação do sistema com base em um \textit{feedback loop}.
Depois do desenvolvimento de um sistema modular como descrito no Capítulo~\ref{ch:mecanismo-de-busca-semantica}, torna-se
fácil configurar os componentes do sistema para ajustá-los com base em resultados de experimentos.
Os experimentos tiveram um objetivo de entender o funcionamento corrente do sistema e ajustar seus componentes
para melhorar sua eficiência, em termos de relevância dos documentos retornados em uma consulta (\textbf{qualidade})
e em termos de \textbf{velocidade} para retornar tais documentos.

Inicialmente, comparo a eficiência de diferentes abordagens de implementação dos componentes do sistema de recuperação
de informação com dados de teses obtidas no repositório de teses da Universidade de São Paulo.
Em seguida, descrevo um experimento usado para comparar o novo sistema com o sistema de recuperação usado no \textit{website}
do repositório de teses da Universidade de São Paulo.

\section{Melhorando a Indexação}
No primeiro experimento que realizei, tentei usar toda a informação de alguns .pdfs selecionados das teses para criar
embeddings.
Além de ter sido demorado para criar cada um embedding, a criação de um embedding por documento se mostrou insatisfatória
para a busca, dada que muita informação era perdida, além de que o documento tinha muita sujeira, o que exige trabalho
extra de limpeza manual.
Essa abordagem resultou em um sistema lento para ser criado e trabalhoso.
Ou seja, não resultou em um bom sistema para fazer a busca.

No segundo experimento que realizei, usei apenas as informações de metadados contidas no site de teses, sem utilizar a tese em si. Os metadados são título, resumo e palavras-chave. Juntei esses 3 em um texto só para gerar cada embedding. O tempo de criação de tudo é em torno de alguns minutos. Com isso, fui capaz de criar um sistema de busca que abarcava todas as teses do site de tese da USP. O sistema foi capaz de obter resultados em menos de 1 segundo. Entretanto, notou-se que a busca foi insatisfatória em algumas instâncias.

Para demonstrar isso, estabeleci o texto de busca "ddos attack". A busca encontrou o vetor mais similar em 0.19s e o resultado retornado foi:

1. Title:  Mitigating DDoS attacks on IoT through machine learning and network functions virtualization
Author:  Oliveira, Guilherme Werneck de
Minha suposição é de que, se esse valor retornado é o mais relevante para "ddos attack", ele deve também ser o mais relevante, ou pelo menos perder para algum mais relevante ainda, para "ddos attack and machine learning". Mas não foi isso que aconteceu, ao buscar por "ddos attack and machine learning", a busca encontrou:

1. Title:  Machine learning in complex networks
Author:  Breve, Fabricio Aparecido
2. Title:  Architecture and development of a real-time multiple content generator system for video games
Author:  Pereira, Leonardo Tortoro
3. Title:  Performance prediction of application executed on GPUs using a simple analytical model and machine learning techniques
Author:  González, Marcos Tulio Amarís
4. Title:  Mitigating DDoS attacks on IoT through machine learning and network functions virtualization
Author:  Oliveira, Guilherme Werneck de
Mesmo que sem utilizar métodos formais de avaliação, nota-se que o sistema de busca não foi relevante ao buscar por "ddos attack and machine learning". Minha suposição é de que muita informação era perdida ao criar um único embedding para todos os metadados.

No terceiro experimento que realizei, utilizei os mesmos metadados, mas decidi criar os embeddings a partir de cada sentença, e não usando todos os metadados ao mesmo tempo. Essa abordagem resultou em buscas em torno de 50 vezes mais lentas, mas bem mais relevantes.

Considere novamente o texto "ddos attack". A busca demorou 4s e o resultado foi:

1. Title:  Method for mitigating against distributed denial of service attacks using multi-agent system.
Author:  Pereira, João Paulo Aragão (Catálogo USP)
2. Title:  A collaborative architecture against DDOS attacks for cloud computing systems.
Author:  Almeida, Thiago Rodrigues Meira de (Catálogo USP)
3. Title:  Mitigating DDoS attacks on IoT through machine learning and network functions virtualization
Author:  Oliveira, Guilherme Werneck de (Catálogo USP)
Comparado ao resultado anterior, eu consideraria que esses resultados foram tão relevantes quanto para essa busca genérica específica. Para saber mais, eu precisaria comparar os 15 primeiros resultados, por exemplo.

Como segundo teste, busquei o texto "ddos attack and machine learning" e o resultado foi:

1. Title:  Mitigating DDoS attacks on IoT through machine learning and network functions virtualization
Author:  Oliveira, Guilherme Werneck de (Catálogo USP)
2. Title:  Reconfigurable learning system for classification of data using parallel processing
Author:  Moreira, Eduardo Marmo (Catálogo USP)
3. Title:  A collaborative architecture against DDOS attacks for cloud computing systems.
Author:  Almeida, Thiago Rodrigues Meira de (Catálogo USP)
Ou seja, pode-se considerar que houve uma melhora considerável de relevância para essa busca específica.

Preliminarmente, notou-se um trade-off entre a qualidade da busca e o tempo levado para buscar e criar o sistema. O sistema que tomou mais para buscar e para criar o sistema resultou em uma qualidade de busca maior. A ideia é que esse sistema seja melhorado em duas frentes, embeddings e busca vetorial. Espera-se que os embeddings resultem em uma maior relevância do resultado de busca e que a busca vetorial resulte em uma maior eficiência (considerando a velocidade do sistema). Afinal, espera-se que um sistema de busca eficiente na web seja capaz de retornar buscas relevantes de forma rápida.



\section{Medindo a qualidade do sistema}
% (NOTE: 1.2 em diante)
% Devem ser propostos experimentos, portanto, para formalizar a avaliação da qualidade da recuperação.
% Com esses experimentos, busca-se criar um conjunto de consultas e relacionar essas consultas a uma coleção de documentos que devem ser retornados.

\chapter*[Conclusão]{Conclusão}
\addcontentsline{toc}{chapter}{Conclusão}


% ----------------------------------------------------------
% ELEMENTOS PÓS-TEXTUAIS
% ----------------------------------------------------------
\postextual
% ----------------------------------------------------------

\bibliography{abntex2-modelo-references}

% ----------------------------------------------------------
% Glossário
% ----------------------------------------------------------
%
% Consulte o manual da classe abntex2 para orientações sobre o glossário.
%
%\glossary

% ----------------------------------------------------------
% Apêndices
% ----------------------------------------------------------
% % ---
% % Inicia os apêndices
% % ---
% \begin{apendicesenv}
% % Imprime uma página indicando o início dos apêndices
% \partapendices
% % ----------------------------------------------------------
% \chapter{Quisque libero justo}
% % ----------------------------------------------------------
% \lipsum[50]
% % ----------------------------------------------------------
% \chapter{Nullam elementum urna vel imperdiet sodales elit ipsum pharetra ligula
% ac pretium ante justo a nulla curabitur tristique arcu eu metus}
% % ----------------------------------------------------------
% \lipsum[55-57]
% \end{apendicesenv}
% % ---

% ----------------------------------------------------------
% Anexos
% ----------------------------------------------------------
% % ---
% % Inicia os anexos
% % ---
% \begin{anexosenv}
% % Imprime uma página indicando o início dos anexos
% \partanexos
% % ---
% \chapter{Morbi ultrices rutrum lorem.}
% % ---
% \lipsum[30]
% % ---
% \chapter{Cras non urna sed feugiat cum sociis natoque penatibus et magnis dis
% parturient montes nascetur ridiculus mus}
% % ---
% \lipsum[31]
% % ---
% \chapter{Fusce facilisis lacinia dui}
% % ---
% \lipsum[32]
% \end{anexosenv}

%---------------------------------------------------------------------
% INDICE REMISSIVO
%---------------------------------------------------------------------
%\phantompart
\printindex
%---------------------------------------------------------------------

\end{document}