% Próximos passos:
%
%   3. Rodar experimentos preliminares de relevância. (1a sem. Setembro)
%   4. Aprender sobre Índice Invertido e descrever sua implementação. (2-3a sem. Setembro)
%   5. Aprender sobre Indexação de Embeddings e descrever a implementação desses índices. (2-3a sem. Setembro)
%   6. Criar índices para LLM mais performático em (3). (4a sem. Setembro)
%   7. Criar índices para busca com BM25 (usar weaviate). (4a sem. Setembro)
%   8. Realizar experimentos com BM25 e LLMs. (4a sem. Setembro)
%   9. Melhorar o sistema de recuperação avaliando o resultado de 8 (1a sem. Setembro)
%   10. Subir o experimento e coletar consultas e rankings de documentos. (1a sem. Outubro)
%   11. Comparar o resultado do sistema de teses e do meu sistema para as consultas coletadas em 10 (2a sem. Outubro)
%   12. Terminar relatórios.


% ------------------------------------------------------------------------
% ------------------------------------------------------------------------
% abnTeX2: Modelo de Trabalho Academico (tese de doutorado, dissertacao de
% mestrado e trabalhos monograficos em geral) em conformidade com
% ABNT NBR 14724:2011: Informacao e documentacao - Trabalhos academicos -
% Apresentacao
% ------------------------------------------------------------------------
% ------------------------------------------------------------------------

\documentclass[
	% -- opções da classe memoir --
	12pt,				% tamanho da fonte
	openright,			% capítulos começam em pág ímpar (insere página vazia caso preciso)
	oneside,			% para impressão em verso e anverso. Oposto a oneside
	a4paper,			% tamanho do papel.
	% -- opções da classe abntex2 --
	%chapter=TITLE,		% títulos de capítulos convertidos em letras maiúsculas
	%section=TITLE,		% títulos de seções convertidos em letras maiúsculas
	%subsection=TITLE,	% títulos de subseções convertidos em letras maiúsculas
	%subsubsection=TITLE,% títulos de subsubseções convertidos em letras maiúsculas
	% -- opções do pacote babel --
	english,			% idioma adicional para hifenização
	french,				% idioma adicional para hifenização
	spanish,			% idioma adicional para hifenização
	brazil				% o último idioma é o principal do documento
	]{abntex2}

% ---
% Pacotes básicos
% ---
\usepackage{lmodern}			% Usa a fonte Latin Modern
\usepackage[T1]{fontenc}		% Selecao de codigos de fonte.
\usepackage[utf8]{inputenc}		% Codificacao do documento (conversão automática dos acentos)
\usepackage{lastpage}			% Usado pela Ficha catalográfica
\usepackage{indentfirst}		% Indenta o primeiro parágrafo de cada seção.
\usepackage{color}				% Controle das cores
\usepackage{graphicx}			% Inclusão de gráficos
\usepackage{microtype} 			% para melhorias de justificação
\usepackage{amsmath}
% ---

% ---
% Pacotes adicionais, usados apenas no âmbito do Modelo Canônico do abnteX2
% ---

\bibliography{abntex2-modelo-references}

% ---
% Pacotes de citações
% ---
\usepackage[brazilian,hyperpageref]{backref}	 % Paginas com as citações na bibl
\usepackage[num]{abntex2cite}
\citebrackets[]

% citar com
% \cite{}
% \citeonline{}
% \citeauthoronline{}
% \citeyear{}
% \citebrackets() é possível!

\usepackage{amsmath}
\usepackage{biblatex}	% Citações padrão ABNT

\addbibresource{books.bib} %Imports bibliography file
\addbibresource{articles.bib} %Imports bibliography file
\addbibresource{online.bib} %Imports bibliography file
\addbibresource{misc.bib} %Imports bibliography file
\addbibresource{inproceedings.bib} %Imports bibliography file
% \addbibresource{thesis.bib} %Imports bibliography file

% ---
% CONFIGURAÇÕES DE PACOTES
% ---

% ---
% Configurações do pacote backref
% Usado sem a opção hyperpageref de backref
\renewcommand{\backrefpagesname}{Citado na(s) página(s):~}
% Texto padrão antes do número das páginas
\renewcommand{\backref}{}
% Define os textos da citação
\renewcommand*{\backrefalt}[4]{
	\ifcase #1 %
		Nenhuma citação no texto.%
	\or
		Citado na página #2.%
	\else
		Citado #1 vezes nas páginas #2.%
	\fi}%
% ---

% ---
% Informações de dados para CAPA e FOLHA DE ROSTO
% ---
\titulo{Título}
\autor{Henrique Araújo de Carvalho}
\local{São Paulo, Brasil}
\data{2023}
\orientador{Daniel Macêdo Batista}
%\coorientador{Coorientador}
\instituicao{%
  Universidade de São Paulo
  \par
  Instituto de Matemática e Estatística
  \par
  Graduação}
\tipotrabalho{Trabalho de Formatura}
% O preambulo deve conter o tipo do trabalho, o objetivo,
% o nome da instituição e a área de concentração
%\preambulo{Preâmbulo.}
% ---

% ---
% Configurações de aparência do PDF final

% alterando o aspecto da cor azul
\definecolor{blue}{RGB}{41,5,195}

% informações do PDF
\makeatletter
\hypersetup{
     	%pagebackref=true,
		pdftitle={\@title},
		pdfauthor={\@author},
    	pdfsubject={\imprimirpreambulo},
	    pdfcreator={LaTeX with abnTeX2},
		pdfkeywords={abnt}{latex}{abntex}{abntex2}{trabalho acadêmico},
		colorlinks=true,       		% false: boxed links; true: colored links
    	linkcolor=blue,          	% color of internal links
    	citecolor=blue,        		% color of links to bibliography
    	filecolor=magenta,      		% color of file links
		urlcolor=blue,
		bookmarksdepth=4
}
\makeatother
% ---

% ---
% Espaçamentos entre linhas e parágrafos
% ---

% O tamanho do parágrafo é dado por:
\setlength{\parindent}{1.3cm}

% Controle do espaçamento entre um parágrafo e outro:
\setlength{\parskip}{0.2cm}  % tente também \onelineskip

% ---
% compila o indice
% ---
\makeindex
% ---

% ----
% Início do documento
% ----
\begin{document}

% Retira espaço extra obsoleto entre as frases.
\frenchspacing

% ----------------------------------------------------------
% ELEMENTOS PRÉ-TEXTUAIS
% ----------------------------------------------------------
% \pretextual

% ---
% Capa
% ---
\imprimircapa
% ---

% ---
% Folha de rosto
% (o * indica que haverá a ficha bibliográfica)
% ---
\imprimirfolhaderosto*
% ---

% ---
% Inserir a ficha bibliografica
% ---

% Isto é um exemplo de Ficha Catalográfica, ou ``Dados internacionais de
% catalogação-na-publicação''. Você pode utilizar este modelo como referência.
% Porém, provavelmente a biblioteca da sua universidade lhe fornecerá um PDF
% com a ficha catalográfica definitiva após a defesa do trabalho. Quando estiver
% com o documento, salve-o como PDF no diretório do seu projeto e substitua todo
% o conteúdo de implementação deste arquivo pelo comando abaixo:
%
% \begin{fichacatalografica}
%     \includepdf{fig_ficha_catalografica.pdf}
% \end{fichacatalografica}
\begin{fichacatalografica}
	\vspace*{\fill}					% Posição vertical
	\hrule							% Linha horizontal
	\begin{center}					% Minipage Centralizado
	\begin{minipage}[c]{12.5cm}		% Largura

	\imprimirautor

	\hspace{0.5cm} \imprimirtitulo  / \imprimirautor. --
	\imprimirlocal, \imprimirdata-

	\hspace{0.5cm} \pageref{LastPage} p. : il. (algumas color.) ; 30 cm.\\

	\hspace{0.5cm} \imprimirorientadorRotulo~\imprimirorientador\\

	\hspace{0.5cm}
	\parbox[t]{\textwidth}{\imprimirtipotrabalho~--~\imprimirinstituicao,
	\imprimirdata.}\\

	\hspace{0.5cm}
		1. Palavra-chave1.
		2. Palavra-chave2.
		I. Orientador.
		II. Universidade xxx.
		III. Faculdade de xxx.
		IV. Título\\

	\hspace{8.75cm} CDU 02:141:005.7\\

	\end{minipage}
	\end{center}
	\hrule
\end{fichacatalografica}
% ---

% % ---
% % Inserir errata
% % ---
% \begin{errata}
% Elemento opcional da \citeonline[4.2.1.2]{NBR14724:2011}. Exemplo:

% \vspace{\onelineskip}

% FERRIGNO, C. R. A. \textbf{Tratamento de neoplasias ósseas apendiculares com
% reimplantação de enxerto ósseo autólogo autoclavado associado ao plasma
% rico em plaquetas}: estudo crítico na cirurgia de preservação de membro em
% cães. 2011. 128 f. Tese (Livre-Docência) - Faculdade de Medicina Veterinária e
% Zootecnia, Universidade de São Paulo, São Paulo, 2011.

% \begin{table}[htb]
% \center
% \footnotesize
% \begin{tabular}{|p{1.4cm}|p{1cm}|p{3cm}|p{3cm}|}
%   \hline
%    \textbf{Folha} & \textbf{Linha}  & \textbf{Onde se lê}  & \textbf{Leia-se}  \\
%     \hline
%     1 & 10 & auto-conclavo & autoconclavo\\
%    \hline
% \end{tabular}
% \end{table}

% \end{errata}
% % ---

% ---
% Inserir folha de aprovação
% ---

% Isto é um exemplo de Folha de aprovação, elemento obrigatório da NBR
% 14724/2011 (seção 4.2.1.3). Você pode utilizar este modelo até a aprovação
% do trabalho. Após isso, substitua todo o conteúdo deste arquivo por uma
% imagem da página assinada pela banca com o comando abaixo:
%
% \includepdf{folhadeaprovacao_final.pdf}
% %
% \begin{folhadeaprovacao}

%   \begin{center}
%     {\ABNTEXchapterfont\large\imprimirautor}

%     \vspace*{\fill}\vspace*{\fill}
%     \begin{center}
%       \ABNTEXchapterfont\bfseries\Large\imprimirtitulo
%     \end{center}
%     \vspace*{\fill}

%     \hspace{.45\textwidth}
%     \begin{minipage}{.5\textwidth}
%         \imprimirpreambulo
%     \end{minipage}%
%     \vspace*{\fill}
%    \end{center}

%    Trabalho aprovado. \imprimirlocal, 24 de novembro de 2012:

%    \assinatura{\textbf{\imprimirorientador} \\ Orientador}
%    \assinatura{\textbf{Professor} \\ Convidado 1}
%    \assinatura{\textbf{Professor} \\ Convidado 2}
%    %\assinatura{\textbf{Professor} \\ Convidado 3}
%    %\assinatura{\textbf{Professor} \\ Convidado 4}

%    \begin{center}
%     \vspace*{0.5cm}
%     {\large\imprimirlocal}
%     \par
%     {\large\imprimirdata}
%     \vspace*{1cm}
%   \end{center}

% \end{folhadeaprovacao}
% % ---

% % ---
% % Dedicatória
% % ---
% \begin{dedicatoria}
%    \vspace*{\fill}
%    \centering
%    \noindent
%    \textit{ Este trabalho é dedicado às crianças adultas que,\\
%    quando pequenas, sonharam em se tornar cientistas.} \vspace*{\fill}
% \end{dedicatoria}
% % ---

% ---
% Agradecimentos
% ---
\begin{agradecimentos}
Agradecimentos.
\end{agradecimentos}
% ---

% % ---
% % Epígrafe
% % ---
% \begin{epigrafe}
%     \vspace*{\fill}
% 	\begin{flushright}
% 		\textit{``Não vos amoldeis às estruturas deste mundo, \\
% 		mas transformai-vos pela renovação da mente, \\
% 		a fim de distinguir qual é a vontade de Deus: \\
% 		o que é bom, o que Lhe é agradável, o que é perfeito.\\
% 		(Bíblia Sagrada, Romanos 12, 2)}
% 	\end{flushright}
% \end{epigrafe}
% % ---

% ---
% RESUMOS
% ---

% resumo em português
\setlength{\absparsep}{18pt} % ajusta o espaçamento dos parágrafos do resumo
\begin{resumo}
 Segundo a \citeonline[3.1-3.2]{NBR6028:2003}, o resumo deve ressaltar o
 objetivo, o método, os resultados e as conclusões do documento. A ordem e a extensão
 destes itens dependem do tipo de resumo (informativo ou indicativo) e do
 tratamento que cada item recebe no documento original. O resumo deve ser
 precedido da referência do documento, com exceção do resumo inserido no
 próprio documento. (\ldots) As palavras-chave devem figurar logo abaixo do
 resumo, antecedidas da expressão Palavras-chave:, separadas entre si por
 ponto e finalizadas também por ponto.

 \textbf{Palavras-chaves}: latex. abntex. editoração de texto.
\end{resumo}

% resumo em inglês
\begin{resumo}[Abstract]
 \begin{otherlanguage*}{english}
   This is the english abstract.

   \vspace{\onelineskip}

   \noindent
   \textbf{Key-words}: latex. abntex. text editoration.
 \end{otherlanguage*}
\end{resumo}

% ---
% inserir lista de ilustrações
% ---
%\pdfbookmark[0]{\listfigurename}{lof}
%\listoffigures*
%\cleardoublepage
% ---
% ---
% inserir lista de tabelas
% ---
%\pdfbookmark[0]{\listtablename}{lot}
%\listoftables*
%\cleardoublepage
% ---
% ---
% inserir lista de abreviaturas e siglas
% ---
%\begin{siglas}
%  \item[ABNT] Associação Brasileira de Normas Técnicas
%  \item[abnTeX] ABsurdas Normas para TeX
%\end{siglas}
% ---
% ---
% inserir lista de símbolos
% ---
%\begin{simbolos}
%  \item[$ \Gamma $] Letra grega Gama
%  \item[$ \Lambda $] Lambda
%  \item[$ \zeta $] Letra grega minúscula zeta
%  \item[$ \in $] Pertence
%\end{simbolos}
% ---
% ---
% inserir o sumario
% ---
\pdfbookmark[0]{\contentsname}{toc}
\tableofcontents*
\cleardoublepage
% ---

% ----------------------------------------------------------
% ELEMENTOS TEXTUAIS
% ----------------------------------------------------------
\textual

\chapter*[Introdução]{Introdução}
\addcontentsline{toc}{chapter}{Introdução}
\label{chap:intro}

% EM CONSTRUÇÃO -------------------------------------------------------------------------------------------------------------------------



O propósito desse trabalho é desenvolver um sistema de recuperação de trabalhos acadêmicos, com ênfase em técnicas modernas
de recuperação desse tipo de documento.

*Definir aqui previamente o problema de ranqueamento *

Descrevo o desenvolvimento desse sistema comparando duas técnicas para recuperação e ranqueamento de documentos.
A primeira técnica, baseada em busca por palavras-chave, é a base da implementação de sistemas de recuperação de informação tradicionais.
A segunda técnica, baseada em busca por \textit{embeddings} (``EBR -- Embeddings Based Search''), é como a implementação de novos sistemas de recuperação
tem sido feita recentemente.

Minha contribuição com o presente trabalho é traçar as bases da criação de um sistema moderno de recuperação de trabalhos acadêmicos nacionais.
A forma que utilizo para medir o sucesso do meu sistema é compará-lo com o sistema de busca por palavras-chave da Biblioteca Digital
de Teses e Dissertações da Universidade de São Paulo \footnote{https://www.teses.usp.br/}.

No capítulo \ref{ch:recuperacao-de-informacao} discuto ...


\chapter{Referencial Teórico}\label{ch:sistemas-de-recuperacao-de-informacao}

\section{Recuperação de Informação}\label{sec:sobre-recuperacao-de-informacao}

Recuperação de informação (``IR -- Information Retrieval'') pode assumir diferentes definições.
Para este trabalho, IR é definida como a disciplina que estuda como encontrar documentos relevantes, de natureza desestruturada, em meio a uma coleção de documentos, a
partir de uma consulta \cite{manning2008introduction}.

Documento, no contexto de IR, geralmente referencia um texto que se pretende recuperar.
No caso deste trabalho, documento se refere a um pedaço específico de texto usado para recuperação, ou ranqueamento
\footnote{Em \citeauthor{bible}, uma definição para documento é ``unidade 'atômica' de ranqueamento''. Essa definição é ainda mais precisa.}.
Especificamente, documentos são os títulos, resumos e palavras-chave de teses acadêmicas, ou partes destes, como sentenças, já que planeja-se usar esses dados para encontrar os trabalhos relevantes.
Presume-se que se um documento é relevante, o trabalho associado a este documento também é relevante.

Documentos relevantes são aqueles que satisfazem a necessidade do usuário \cite{manning2008introduction}.
A definição de relevância é específica de um problema e depende de fatores como
características da busca, do usuário a que ela se destina, da consulta, dos documentos, e de
tempo e espaço da busca, o que confere à relevância uma natureza subjetiva e dinâmica.
Como explicam \citeauthor{Ceri2013}, ``a relevância é multifacetada, sendo determinada não só pelo conteúdo de um
resultado recuperado, mas também por aspectos como autoridade, credibilidade, especificidade, exaustividade,
atualidade e clareza de sua fonte''.

Documentos são de natureza desestruturada quando não possuem um modelo de dados definido, ou que são difíceis para um computador
estruturar, interpretar, analisar ou segmentar \cite{manning2008introduction}.
Exemplos de dados desestruturados são texto, imagem, áudio e vídeo, que são encontrados na forma de
documentos como páginas da web, artigos científicos, imagens, filmes, dentre outros formatos.
Em contraste, documentos estruturados são os que possuem uma estrutura relacional clara ou são fáceis para um computador estruturar.
A disciplina de estudo de dados estruturados é a recuperação de dados, que se baseia em uma forma expressa e bem
definida de recuperação, articulada de maneira formal, sobre informação que possui um modelo pré-definido de dados \cite{JurafskyMartin2023}.

A IR é tratada como um campo de conhecimento que abrange uma variedade de
sistemas, aqui chamados de sistemas de recuperação de informação (``IRS -- Information Retrieval System''), que se manifestam de diferentes formas.
Por exemplo, IRSs são componentes centrais em motores de busca,
sistemas de filtragem de informação, sistemas de sumarização de documentos, sistemas de perguntas e respostas,
sistemas de recomendação, dentre outros \cite{Ceri2013}.

\section{Sistema de Recuperação de Informação}\label{sec:sobre-recuperacao-de-informacao}

Defino o IRS como a implementação de um modelo de recuperação de informação (``IRM -- Information Retrieval Model'').
Um IRM descreve o funcionamento da recuperação de informação em um sistema e é formalmente definido pela quádrupla: \cite{Ceri2013}

\begin{equation}
\text{IRM} = \{D, Q, F, R(q_k, d_j)\}
\label{eq:irm}
\end{equation}

onde
\begin{itemize}
    \item \(D\) é um conjunto de representações \(d_j\) de documentos;
    \item \(Q\) é um conjunto de representações \(q_k\) de consultas;
    \item \(F\) é a estratégia adotada para modelar a representação de documentos e consultas e suas relações;
    \item \(R(q_k, d_j)\) é a função de ranqueamento que associa \(d_j\) a um número real que denota sua relevância para \(q_k\).
\end{itemize}

Autores costumam categorizar modelos em diferentes abordagens, a depender de como são definidos os elementos $D$, $Q$, $F$ e $R$ do modelo.
Em essência, modelos são categorizados a partir de seu entendimento sobre relevância e da estratégia adotada para recuperar documentos relevantes.

Modelos que se encaixam na abordagem de correspondência exata compreendem relevância e adotam uma estratégia para recuperar documentos fundamentalmente diferente da estratégia adotada por modelos que se encaixam ou na abordagem de espaço vetorial, ou de modelos que se encaixam na abordagem probabilística
\cite{Hiemstra2009}.

% Importante notar que não há uma forma precisa, ou mesmo uma preocupação, de categorizar fielmente um modelo
% \footnote{Por exemplo, algumas abordagens probabilísticas de representar documentos, como BM-25, são também uma forma abordagem de espaço vetorial.}
% e que não existe uma abordagem ou um modelo melhor que outros para implementação de um sistema.
Não é incomum, entretanto, encontrar descrições de modelos que se encaixam em diferentes abordagens,
de modo que, tanto não parece existir um consenso de classificação dos diferentes modelos
\footnote{Alguns autores descrevem modelos baseados em comparação de palavras-chave como modelos de correspondência exata \cite{} e outros os descrevem como modelos de espaço vetorial \cite{Hiemstra2009}.}, quanto
um modelo pode carregar características de diferentes abordagens sem que isso descaracterize sua abordagem principal.
Por exemplo, modelos probabilísticos podem usar conceitos de modelos de correspondência exata para encontrar documentos relevantes e usar conceitos de modelo de espaço vetorial para representar seus documentos.

Cumpre observar, apenas, que a definição de um IRM é importante para delinear e conduzir experimentos e constatar sua eficácia para uma aplicação específica.
A descrição formal do modelo é necessária para garantir consistência e para ``garantir que o modelo pode ser implementado em um sistema real'' \cite{Hiemstra2009}.

Ainda, ao mesmo tempo que um modelo é responsável por ser o esboço ou plano (\textit{blueprint}) de um IRS, um IRS também pode implementar diferentes modelos.
Como exemplo, o buscador do Google permite buscas por correspondências exatas de palavras-chave e booleanas
\footnote{https://support.google.com/websearch/answer/2466433}
e, ao mesmo tempo, implementa outros modelos que permite encontrar maior relevância em buscas cotidianas, como o PageRank ou modelos baseados em redes neurais
\footnote{https://searchengineland.com/how-google-uses-artificial-intelligence-in-google-search-379746}.

Ao fim, a implementação de um IRS e os modelos escolhidos para serem implementados, dependerão das características dos documentos, das especificidades das consultas e da estratégia empregada na representação, recuperação e ranqueamento de um documento na coleção.
Principalmente, a maneira que um IRM compreende relevância influencia diretamente na definição de seus elementos.

E se, como argumentado, a definição de relevância é específica de um problema, então a escolha do IRM implementado por um IRS será primordialmente pautada pelo resultado que se pretende alcançar, considerando as características reais dos documentos, consultas e usuários.
Ou seja, a validade e eficácia do IRM é julgada pelos resultados obtidos após sua tradução em IRS, avaliados em um contexto específico, definido um objetivo específico.

Portanto, antes da implementação de qualquer IRS, é necessário definir o problema que pretendo estudar nesse trabalho, o objetivo pretendo alcançar e os modelos que pretendo implementar.



\section{O Problema de Ranqueamento, Objetivo e Escopo do Trabalho}\label{sec:o-problema-de-ranqueamento}

O problema de ranqueamento de documentos pode ser definido como, ``dada uma necessidade de informação expressada por meio da consulta $q$, a tarefa de [...] retornar uma lista
ranqueada de $k$ textos $\{d_1, d_2 ... d_k\}$ de uma coleção de textos $C = \{d_i\}$ arbitrariamente grande, mas finita, que maximiza uma métrica de interesse, por exemplo, nDCG, AP, etc''\cite{biblia}.

Um IRM busca solucionar o problema de ranqueamento através de uma função de ranqueamento \(R(q_i, d_j)\) que associa \(d_j\) a um número real $s_j$ que denota sua relevância para \(q_i\).
A lista ranqueada de textos pode ser ``mais explicitamente caracterizada como $\{(d_1, s_1), (d_2, s_2) ... (d_k, s_k)\}$ com a restrição de que $s_1 > s_2 > ... > s_k$'' \cite{biblia}.

O ranqueamento pode ser entendido como um problema específico dentro da recuperação de informação. Assim, ao dizer que, com este trabalho, proponho desenvolver um sistema de recuperação de literatura acadêmica brasileira, quero dizer que proponho desenvolver um sistema de ranqueamento de literatura acadêmica brasileira.
A ideia deste trabalho não é esgotar todas as técnicas atuais para ranqueamento de documentos ou usar todas as melhores práticas para criar um sistema completo de busca.
Pretendo estudar e comparar a eficácia da implementação de dois IRMs, usando ferramentas disponíveis ao público.

A motivação deste trabalho é compreender a eficácia e o uso de técnicas modernas de busca, baseadas em modelos de redes neurais, em comparação com técnicas tradicionais baseadas em correspondência de palavras.
Ao longo deste texto, eu mostro como criar um simples sistema capaz de fornecer resultados que permitem avaliar a eficácia de diferentes modelos para os dados com que se trabalha.
Tudo isso usando um conjunto de dados de tamanho mediano, aproximadamente 106 mil trabalhos acadêmicos, que resultam em aproximadamente 1,3 milhão de documentos.

O primeiro IRM, chamado aqui de \textbf{modelo de correspondência exata}, representa documentos e consultas por meio de vetores esparsos e utiliza funções probabilísticas para comparar esses vetores e realizar ranqueamento. O segundo, chamado aqui de \textbf{modelo de busca semântica}, representa documentos e consultas por meio de vetores densos e utiliza operações algébricas para ranqueamento.

Nesses dois IRMs, a função de ranqueamento busca estimar a relevância de um documento $d$ em relação a uma consulta $q$, por meio de uma função $S$ que calcula a similaridade entre as representações de $q$ e $d$, $\varphi_q$ e $\varphi_d$, respectivamente:

\begin{equation}\label{eq:irm-rank}
    R(q, d) = S(\varphi_q(q), \varphi_d(d))
\end{equation}

Entendo que ambos os modelos podem ser considerados variações da abordagem de modelos de espaços vetoriais, uma vez que
usam representações para documentos e consultas baseados em uma distribuição estatística dos termos na coleção de documentos.
Entretanto, ambos são fundamentalmente diferentes na forma que criam essas representações dos documentos e consultas e na forma que calculam o ranqueamento, ou seja, na forma que atribuem relevância.
No restante do capítulo justifico a estratégia usada por esses IRMs para representar documentos e consultas e, em seguida, explico suas diferenças na forma de abordar essa estratégia.
Na seção \ref{sec:representacao-vetorial-de-texto}, discuto como documentos e consultas são representados nesses IRMs.
Na seção \ref{sec:ranqueamento-de-documentos}, discuto como documentos são ranqueados nesses IRMs.
Na seção \ref{sec:arquitetura-de-um-sistema-de-recuperacao-de-informacao}, discuto os aspectos gerais de um IRS sob a ótica dos IRMs propostos.

\section{Os Modelos de Recuperação de Informação Estudados}\label{sec:representacao-vetorial-de-texto}

Semântica vetorial é o nome que se dá à forma padrão de representar texto por meio de vetores.
A base da atual semântica vetorial nasceu nos anos 50, quando foram propostas ideias de representar
o significado de uma palavra por meio de sua distribuição no uso da linguagem e de representar essa distribuição por meio de um vetor
\cite{JurafskyMartin2023}.

Essa abordagem é chamada de hipótese distribucional, que estipula que palavras que ocorrem em um mesmo contexto possuem significados similares \cite{PilehvarCamacho-Collados2022}.
Aqui, possuir ``significado similar'' significa que seu uso na linguagem é similar, que ela ocorre nas mesmas posicões em uma frase ou que mantém relação parecida com as demais palavras.
Palavras que ocorrem em um mesmo contexto, teriam, assim, similaridade semântica.
Talvez elas sejam palavras sinônimas, ou talvez elas carreguem algum sentido comum quando usadas em um contexto específico.
\footnote{Cumpre notar que algumas palavras que carregam similaridade semântica em um contexto podem simplesmente não ter similaridade em outro contexto.}
Sob a mesma premissa, documentos que possuem distribuição parecida de palavras são considerados similares.

A ideia por trás da semântica vetorial é representar palavras como um ponto em um espaço multidimensional derivado das distribuições de vizinhanças, de palavras e palavras que estão próximas a elas,
possibilitando a medida de um grau de similaridade de termos ou documentos por meio de um cálculo de similaridade entre suas representações vetoriais \cite{JurafskyMartin2023}.

As representacões vetoriais podem ser classificadas entre técnicas baseadas na contagem de frequência de ocorrência
ou co-ocorrência das palavras, que resultam em representações esparsas de texto usadas no modelo de correspondência exata, e técnicas baseadas em redes neurais, que resultam
em representações densas de texto usadas no modelo de busca semântica.

Explicada a estratégia da representação vetorial de texto, que baseia os IRMs baseados no modelo de espaço vetorial, esclareço as diferenças entre as estratégias usadas pelo modelo de correspondência exata \ref{sec:modelo-de-correspondencia-exata} e pelo modelo de busca semântica \ref{sec:modelo-de-busca-semantica}.

\subsection{Modelo de Correspondência Exata}\label{sec:modelo-de-correspondencia-exata}

Para explicar o modelo de correspondência exata, explico primeiramente a forma usada por esse modelo para representar texto. Em seguida, explico a forma usada por esse modelo para calcular a similaridade entre textos.

Representações esparsas de texto são geralmente baseadas em uma matriz de co-ocorrência, uma forma de representar a ocorrência de um
termo em relação a um documento.
Essas são chamadas de matrizes de termo-documento.
\footnote{Existem também as matrizes que relacionam termos a outros termos (matrizes de termo-termo), mas essas não possuem relevância para este trabalho.}
\cite{JurafskyMartin2023}

Nessas matrizes, um vetor coluna denota um documento, que é representado pelas frequências de seus termos.
Cada linha $i$ da matriz representa uma palavra e cada coluna $j$ representa um documento.
Ou seja, o valor $M_{i,j}$ representa a frequência da palavra $P$ da linha $i$ em um documento $D$ da coluna $j$.
Como exemplo, veja a coleção de documentos abaixo e parte de sua matriz de co-ocorrência termo-documento:

\begin{center}
\begin{tabularx}{\textwidth}{|c|X|}
\hline
Documento & Texto \\
\hline
Documento 1 & Esperou em um banco para pagar a conta. Depois de horas esperando, saiu do banco e foi ao parque. \\
\hline
Documento 2 & Sentou em um banco da praça para aguardar sua chegada. \\
\hline
Documento 3 & Eu banco a despesa. \\
\hline
\end{tabularx}
\end{center}


\begin{table}[ht]
\centering
\caption{Matriz Termo-Documento.}
\label{tab:term-doc-matrix}
\begin{tabular}{|l|c|c|c|}
\hline
Termo     & Documento 1 & Documento 2 & Documento 3 \\
\hline
a         & 1           & 0           & 1           \\
\hline
aguardar  & 0           & 1           & 0           \\
\hline
ao        & 1           & 0           & 0           \\
\hline
banco     & 2           & 1           & 1           \\
\hline
chegada   & 0           & 1           & 0           \\
\hline
\multicolumn{4}{c}{\vdots} \\
\end{tabular}
\end{table}


Em uma matriz $m \times n$, cada vetor de palavra tem dimensão $n$ e cada vetor de documento tem dimensão $m$.
Uma consequência desse tipo de representação é que o número de linhas é igual ao tamanho do vocabulário e o número de colunas é igual ao número de documentos.
Essa representação ocupa um espaço proporcional a $m \times n$ e é considerada esparsa para a maior parte dos documentos, já que a distribuição de palavras por documento é pequena quando comparada ao número de documentos.
\cite{JurafskyMartin2023}

% [COMENTÁRIO]
% Futuramente, cumpre ressaltar que usar essa representação para recuperação de informação
% exige pensar em armazenamento e recuperação de matrizes com essas
% características específicas.
A ideia é que a consulta também seja representada por um vetor esparso de frequências, da mesma forma que um documento, de modo que permita uma comparação entre as duas.
Por esse motivo, a comparação entre uma consulta e um documento é também chamada, indinstintamente, de comparação entre documentos.

IRMs que usam representações esparsas de texto calculam a similaridade entre documentos por meio de uma análise da frequência dos termos presentes na consulta e que também estão presentes nos documentos da coleção.
A similaridade entre documentos pode, assim, ser feita pela análise dos valores de frequência dos vetores de documento.

Entretanto, uma análise crua da frequência de termos por documento pode não ser representativa da similaridade entre documentos.
Primeiro, a alta frequência de uma palavra em um documento pode afetar desproporcionalmente uma comparação entre documentos simplesmente por um documento ter mais palavras que o outro.
Segundo, algumas palavras aparecem com muita frequência em documentos e não
carregam grande significado semântico.
Necessita-se, portanto, de uma medida, baseada nos valores das frequências, que passe a representar
a importância de uma palavra para um documento.

Por esses motivos, a análise de frequência é feita a partir atribuição de pesos aos termos (\textit{weighting}), seguida de alguma medida de associação entre os termos da consulta e os termos do documento.
Assim, uma função de atribuição de valor (função de \textit{scoring}) pode ser descrita como:

\begin{equation}
\label{eq:score}
    S(q,d) = \sum_{t \in q \cap d} w(t)
\end{equation}

onde $w$ é uma função que atribui um peso a um termo $t$, a partir de uma estatística associada ao termo no documento ou coleção.

As estatísticas associadas comumente usadas
\footnote{Essas são comumente usadas para computar variações da família de funções chamadas de frequência de termo e inverso da frequência por documento (tf-idf -- \textit{Term Frequency Inverse Document Frequency}).}
são:
(a) frequência do termo no documento (tf -- \textit{term frequency})
% (equação \ref{eq:tf})
, (b) frequência do termo na coleção de documentos ou seu inverso (df \textit{-- document frequency} e idf { -- \textit{inverse document frequency}})
% (equações \ref{eq:df} e \ref{eq:idf})
, e (c) tamanho do documento (dl \textit{-- document length})
% (equação \ref{eq:dl})
 \cite{biblia}.

\begin{equation}
\text{tf}(t, d) = count(t, d)
\label{eq:tf}
\end{equation}

\begin{equation}
\text{df}(t) = \left| \{ d \in D : t \in d \} \right|
\label{eq:df}
\end{equation}

\begin{equation}
\text{idf}(t) = \log \frac{|D|}{\text{df}(t)}
\label{eq:idf}
\end{equation}

\begin{equation}
\text{dl}(d) = \sum_{t \in d} f(t,d) \\
\label{eq:dl}
\end{equation}

\begin{equation}
\text{avgdl}(d) = \frac{1}{|D|} \sum_{d \in D} \text{dl}(d)
\label{eq:}
\end{equation}

% https://nlp.stanford.edu/IR-book/html/htmledition/okapi-bm25-a-non-binary-model-1.html#14366

A função $w$ mais usada em sistemas de busca por correspondência exata é a função BM25 (\textit{Best Match 25}) \cite{biblia}:

\begin{equation}
\text{BM25}(d,q) = \sum_{t \in q \cap d} \frac{(k_1 + 1) \times \text{tf}(t,d)}{k_1 \times ((1-b) + b \times \frac{\text{dl}(d)}{\text{avgdl}(d)}) + \text{tf}(t,d)} \times \log \frac{|D| - \text{df}(t) + 0.5}{\text{df}(t) + 0.5}
\end{equation}

em que o segundo termo da equação é idf adaptado e $k$ e $b$ são hiperparâmetros.

Variações da BM25 são o estado da arte na representação vetorial esparsas de documentos e, por esse motivo, essa função ainda é largamente implementada em sistemas de recuperação de informações \cite{biblia}.

Uma vantagem desses IRMs é que eles são eficientes, já que o cálculo de relevância de uma consulta tem complexidade proporcional ao número de termos da consulta, e são simples de implementar, já que dependem
apenas da criação de um índice invertido, uma estrutura de dados que, em essência, implementa a matriz termo-documento.
É fácil visualizar cálculo de similaridade nessa estrutura.

No entanto, o cálculo de similaridade baseado em análise de frequências apresenta desvantagens significativas. A matriz exemplificada na Tabela \ref{tab:term-doc-matrix} ilustra as limitações dessa abordagem.

Primeiro, uma representação de um texto aqui descrita não consegue capturar a polissemia de um termo.
No caso exemplo, o termo ``banco'' apresenta 3 significados diferentes e uma consulta ``bancos de parque''
provavelmente teria como retorno mais relevante o Documento 1 (independentemente ta medida de associação a ser considerada),
ao invés do Documento 2, que é factualmente mais relevante para a consulta.

Segundo, e mais importante, esse modelo sofre com a incompatibilidade de vocabulário (\textit{vocabulary mismatch}).
Esse é um problema conhecido em PNL e se manifesta quando ``pessoas diferentes nomeiam a mesma coisa ou conceito de formas diferentes''
% \footnote{https://en.wikipedia.org/wiki/Vocabulary_mismatch}
.
No caso exemplo, a consulta ``pagar os custos'' provavelmente teria como documento mais relevante o Documento 1, o que não é necessariamente verdade,
dado que o Documento 3 é ``Eu banco a despesa'', que contém uma ideia semanticamente mais parecida com a consulta.

Vetores esparsos têm uma boa capacidade de representar documentos para uma comparação por similaridade textual, mas não para similaridade semântica.
Como se pode verificar pela explicação acima, para o caso em que o documento relevante define a uma mesma ideia com diferentes palavras, ou usa palavras sinônimas, mas não uma distribuição de palavras similar à da consulta, uma busca que usa BM25 torna-se insatisfatória. \cite{thakur-2021-BEIR}

A necessidade de correspondência exata é amenizada por meio de algumas técnicas simples de PNL, como expansão da consulta, \textit{stemming} e \textit{lemmatization}, mas ainda assim, essas técnicas não são capazes de superar o problema da incompatibilidade de vocabulário.

% Portanto, o IRM descrito depende da correspondência exata entre os termos da consulta e os termos do texto e acaba enfrentando o desafios de linguagem sem solução simples.

Historicamente, três abordagens têm sido usadas para superar esse desafio \cite{biblia}:
\begin{itemize}
    \item[(a)] melhorar as representações da consulta;
    \item[(b)] melhorar as representações dos documentos; e
    \item[(c)] usar representações não dependentes de correspondência exata.
\end{itemize}

A primeira abordagem é chamada de expansão da consulta.
Nessa técnica, a consulta é expandida através da adição de termos similares aos termos buscados.
Essa é uma técnica eficiente que melhora consideravelmente o resultado da busca por correspondência exata \cite{biblia}.

Uma abordagem que deriva, de certa forma, da abordagem (c), é a utilização de representações textuais densas para calcular a similaridade semântica \cite{biblia}.
De fato, para solucionar os problemas com o atual IRM, necessita-se uma nova forma de representar documentos e consultas, uma que considere o significado semântico dos termos, com menor ênfase na contagem de termos léxicos e maior ênfase em adquirir intuição sobre seu significado no contexto \cite{bhaskar-craswell-2018}.

% [INFORMAÇÃO REMOVIDA]
% Em matrizes termo-termo, termos são relacionados com outros termos que ocorrem no mesmo contexto.
% Nesse caso, cada palavra P ocupa uma linha i e uma coluna j, ou seja, cada vetor de palavra tem dimensão |P|,
% e o valor $Mi,j$ é a frequência em que uma palavra ocorre no mesmo contexto de outra.
% Um contexto é definido como uma janela de $k$ palavras na mesma sentença e a ocorrência no mesmo contexto é medida como
% $(k-1)/2$ palavras à direita e $(k-1)/2$ palavras à esquerda.
% Essa representação ocupa um espaço proporcional a |P| x |P| e também é esparsa.
% \cite{JurafskyMartin2023}

\subsection{Modelo de Busca Semântica}\label{sec:modelo-de-busca-semantica}

Uma solução proposta para o problema observado em abordagens puramente léxicas foi a utilização de redes neurais para criar representações densas de texto (\textit{embeddings}).
Nessas representações, o texto é mapeado para um espaço vetorial de baixa dimensão, em que todas as dimensões têm valores diferentes de 0.
Essas representações são chamadas de densas em oposição às representações esparsas observadas no modelo de correspondência exata, em que a dimensão era proporcional ao tamanho do vocabulário.

O uso de redes neurais para resolver o problema do ranqueamento tem se baseado em duas abordagens, modelos que se valem do rerranqueamento de documentos retornados por um modelo de correspondência exata e modelos que usam técnicas de recuperação profunda (\textit{dense retrieval}), que computam o ranqueamento diretamente nas representações vetoriais capazes de capturar significado semântico \cite{biblia}.

Modelos pré-treinados só passaram a ser usados em problemas de busca e ranqueamento de texto com modelos baseados na arquitetura Transformers \cite{}
\footnote{Como descrevem os autores \citeauthor{}, a partir de Outubro de 2019 alguns artigos de blog do Google e Microsoft reportam a utilização de modelos de linguagem para melhorar a busca em seus serviços.}
\cite{DBLP:journals/corr/abs-1810-04805}
Uma grande vantagens dessas novas arquiteturas é sua generalização para diferentes tarefas de IR e para diferentes conjuntos de dados.
Com isso, é possível usar modelos pré-treinados para alcançar bons resultados em conjuntos de dados diferentes daqueles para os quais os modelos foram treinados \cite{}.

BERT \cite{reimers-2019-sentence-bert} é o mais representativo desta leva de modelos de linguagem.
Como colocado pelos autores \citeauthor{biblia}, BERT se destacou por ``reunir muitos ingredientes cruciais para produzir enormes saltos na eficácia de uma ampla gama de tarefas de processamento de linguagem natural''.
Em um movimento inspirado por BERT, vários outros modelos de linguagem surgiram a partir de 2019, expandindo o estado da arte em tarefas de geração de representações densas de texto.

Diversos modelos foram disponibilizados pela biblioteca Sentence Transformers\footnote{}, implementada em Python.
Desde então, vários outros modelos pré-treinados são disponibilizados na plataforma Hugging Face\footnote{},
apropriados para diferentes tarefas de IR.
Nesse trabalho, são usados modelos pré-treinados open-source da biblioteca Python sentence\_transformers como base para criação dos embeddings.

A primeira abordagem evidentemente sofre com o problema de \textit{vocabulary mismatch} em sua primeira etapa de recuperação de informação. Para além disso, como notam os autores \citeauthor{biblia}, existem outros motivos para preferir a segunda abordagem. Primeiro, o custo computacional de rerranquear documentos é caro e é feito durante a consulta, ao invés de ser feito na etapa de pre-processamento como no modelo de \textit{dense retrieval}, que calcula apenas a inferência da consulta e usa uma função de similaridade rápida de calcular em uma coleção de vetores densos usando soluções baseadas em \textit{nearest neighbor search}. Segundo, arquiteturas de várias camadas são pouco elegantes e trazem uma série de problemas para o treinamento do modelo de rede neural que se localiza na segunda camada.

A explicação da arquitetura ou treinamento desses modelos está além do escopo dessa pesquisa.
Aqui, entende-se esses modelos como uma função que tem como texto como entrada e vetores que representam um texto como saída.

\textbf{[CASO DE TEMPO: COMPLETAR COM UM OVERVIEW DA FORMA QUE FUNCIONA DENSE RETRIEVAL MODELS}]

Portanto, \textit{dense retrieval} soluciona o problema de ranqueamento através da geração de representações densas dos documentos e do cálculo uma métrica simples de similaridade entre a representação da consulta e do documento.
Essa métrica simples é geralmente obtida através de operações de álgebra linear que medem similaridade entre vetores.

Um método algébrico comum de medir a similaridade entre dois vetores é através do produto interno, definido por:

\begin{equation}
\text{dot}(\mathbf{u}, \mathbf{v})  = \mathbf{u} \cdot \mathbf{v} = \sum_{i}u_{i}v_{i}
\label{eq:dot_product}
\end{equation}

Uma interpretação para o resultado do produto interno é a medição de quanto um vetor $u$ está no mesmo sentido e direção de outro vetor $v$. Essa é uma interpretação conveniente para compreender a medida de similaridade entre duas representações vetoriais de texto.

Um problema de realizar medições com o produto interno é comparar similaridades, pois $dot(u,v)$ pode ser arbitrariamente grande ou pequeno a depender dos valores dos componentes $u_i$ e $v_i$ dos vetores. Nesse caso, pode parecer mais sensato calcular o produto interno com vetores normalizados, o que resulta no valor do cosseno do ângulo entre os vetores e leva todas as comparações
a um intervalo entre 0 e 1, uma vez $v_i, u_i \geq 0, \forall i$:

\begin{equation}
\cos (\mathbf{u}, \mathbf{v})  = \frac{\mathbf{u} \cdot \mathbf{v}}{\|\mathbf{u}\| \|\mathbf{v}\|}
\label{eq:cosine_similarity}
\end{equation}

Assim, é possível comparar similaridades entre diferentes vetores de palavras.
Quanto mais próximo de 1 for o resultado, mais similares são os termos comparados.
Essa técnica funciona para qualquer modelo vetorial de texto.

No caso da similaridade entre um documento $d$ e um conjunto de documentos $V$, tal que $V = \{\mathbf{v_1}, \mathbf{v_2}, ... \mathbf{v_n}\}$,
pode-se calcular o cosseno entre a representação de $d$, denotada por $\mathbf{u}$, e o centróide $\mathbf{c}$ de $V$, dado por:

\begin{equation}
\mathbf{c} = \frac{1}{n} \sum_{i=1}^{n} \mathbf{v_i}
\label{eq:centroid}
\end{equation}

Dessa forma, a similaridade entre $d$ e $V$ é dada por $\cos⁡ (\mathbf{u},\mathbf{c})$ .

A escolha da forma de comparação, entretanto, vai depender da arquitetura do modelo e de seu treinamento. Alguns modelos são treinados para serem usados com produto interno, outros com cálculo de cosseno e, para outros, não vai fazer qualquer diferença.

Desde a publicação do texto de referência usado para a elaboração dessa seção \cite{biblia}, diversas soluções de software que cuidam da \textit{pipeline} de \textit{dense retrieval}, popularmente chamadas bases de dados vetoriais, surgiram.
Essas \textit{pipelines} se baseiam na geração de representações densas de texto seguida da comparação de similaridade entre a representação densa de uma consulta e dos textos.
% https://www.youtube.com/watch?v=Mt7UJNKxscA&list=PLSg1mducmHTPZPDoal4m59pPxxsceXF-y&index=7

% - Diferentes associações ou similaridades
% - Analogia / Similaridade Relacional



% ----------------------------------------------------------------------------------------------------------------------
% TÓPICOS:
%- Word2vec: pequenos e densos; skip-gram; SGNS; static embeddings;
% - O problema com Word2Vec e Contextual Embeddings
%- Dynamic Contextual Embeddings
% ----------------------------------------------------------------------------------------------------------------------

% [2]

% Quanto maior a dimensionalidade do embedding maior a qualidade dele.

% OpenAI and Cohere embeddings, which require a paid API call to generate them, can be considered higher quality due to a dimensionality of a few thousand.

% One reason it makes sense to use a paid API to generate embeddings is if your data is multilingual (Cohere is known to possess high-quality multilingual embedding models that **[are known to perform better](https://docs.cohere.com/docs/multilingual-language-models#model-performance)** than open source variants).

% E se, ao invés de criar mais embeddings, eu mudar a dimensionalidade do embedding?

% In his excellent review post**[3](https://thedataquarry.com/posts/vector-db-2/#fn:3)**, Colin Harman describes how a lot of companies, due to the plethora of vector DB marketing material out there today, experience “tunnel vision” when it comes to the search & retrieval landscape. As practitioners, we have to remember that vector databases are not the panacea of search – they are very good at *semantic* search, but in many cases, traditional keyword search can yield more relevant results and increased user satisfaction**[4](https://thedataquarry.com/posts/vector-db-2/#fn:4)**. Why is that? It’s largely to do with the fact that ranking based on metrics like cosine similarity causes results that have a higher similarity score to appear above partial matches that may contain specific input keywords, reducing their relevance to the end user.

% *Cross Encoders Models →* usar depois dos Bi-directional?

% including an agent-based framework like LangChain

% - o que melhora um embedding? Um embedding parece melhorar quando eu divido mais um grande texto. Mas o embedding também piora quando eu diminuo até um nível atômico, então existe um ponto ótimo. Qual o ponto ótimo? Eu acredito que sejam níveis que contêm um núcleo semântico. Então keywords deveriam ser divididas por letras maíusculas em um PDF, a não ser que elas sejam uma sigla. Ainda, nem todas as keywords seguem esse padrão, mas acho que só de pegar alguns já é uma vantagem. As frases podem ser divididas entre ponto. E o título é um ponto só.
% - Os embeddings ignoram stop words? Segundo esse link sim: https://github.com/UKPLab/sentence-transformers/issues/383. Eu consegui replicar isso:
% Outras Referências
% [2]https://thedataquarry.com/posts/vector-db-2/#how-are-embeddings-generated
% [3]https://techcrunch.com/sponsor/nvidia/how-the-revolution-of-natural-language-processing-is-changing-the-way-companies-understand-text/
% [4] https://qdrant.tech/articles/hybrid-search/
% [5] https://colinharman.substack.com/p/beware-tunnel-vision-in-ai-retrieval
% [6] https://thedataquarry.com/posts/vector-db-2/#fn:4

% https://www.sbert.net/examples/domain_adaptation/README.html?highlight=fine%20tuning

% Information retrieval is the process of searching and returning relevant documents for a query from
% a collection. In our paper, we focus on text retrieval and use document as a cover term for text of
% any length in the given collection and query for the user input, which can be of any length as well.
% Traditionally, lexical approaches like TF-IDF and BM25 [ 55 ] have dominated textual information
% retrieval. Recently, there is a strong interest in using neural networks to improve or replace these
% lexical approaches. In this section, we highlight a few neural-based approaches and we refer the
% reader to Lin et al. [37] for a recent survey in neural retrieval
% https://arxiv.org/pdf/2104.08663.pdf

%[COMENTÁRIO: mas isso nao considera o tamanho do documento... Eu entendo que squash com logaritmo acaba normalizando tambem,
%mas a ideia deveria ser normalizar dentro de um documento, pegar a frequencia dentro do documento.]
%pequenas diferenças de frequência de uma palavra entre documentos não deveria representar grandes diferenças
%em similaridade.
%É mais interessante, assim, modificar a escala para graduar uma palavra por sua frequência do documento apenas quando ela
%for extremamente frequente.

% No caso de termos, um motivo.
% Palavras muito frequentes em todos os contextos representam alto ruído (assim como no caso de documentos), mas não carregam
% grande significado semântico.
% Nesse caso, a medida usada para resolver esse problema entre termos é \textit{Pointwise Mutual Information} (PMI), que objetiva
% calcular a probabilidade em que duas palavras aparecem em um contexto em relação à probabilidade que seria esperado encontrar essas palavras em um
% mesmo contexto, dado que elas são independentes.
% Isso é representado por um cálculo de probabilidades, em que p(x,y) é a probabilidade conjunta de aparecerem os termos x e y
% em um mesmo contexto e p(x) e p(y) são as probabilidades marginais de x e y ocorrerem, respectivamente.

% Sabendo que, para t e w termos de uma coleção, $p(t) = \frac{count(t)}{\sum{w}count(w)}$,

% $$\text{PMI}(x, y) = \log \frac{p(x, y)}{p(x) \cdot p(y)}$$

% Como a relação de co-ocorrência entre termos é simétrica, pode-se interpretar as dimensões de um vetor linha da matriz de co-ocorrência
% termo-termo como sendo contextos, o que faz dessa uma matriz de co-ocorrência termo-contexto.
% Assim, para um termo t e um contexto c,

% $$\text{PMI}(t,c) = \log \frac{p(t,c)}{p(t) \cdot p(c)}$$

% Como o valor de PMI pode ser negativo, algo a que não se consegue atribuir um sentido no caso de similaridade entre termo
% e contexto, usa-se a medida Positive PMI (PPMI), calculada como

% $$\text{PPMI}(x, y) = \max \left(0, \log \frac{p(x, y)}{p(x) \cdot p(y)}\right)$$

% Essa medida tem a tendência de ser enviesada para contextos pouco frequentes (imagine p(c) << 1).
% Uma forma de corrigir isso é computar a probabilidade por meio da função $P_{\alpha}(c)$, em que se eleva a probabilidade
% de um contexto à $\alpha$ potência, com o objetivo de
% ``aumentar a probabilidade de contextos raros e diminuir o PMI ($P_{\alpha} > P(c)$ quando $c$ é raro)''
% \cite{JurafskyMartin2023} p. 118.

\section{Arquitetura de um Sistema de Recuperação de Informação}
\label{sec:arquitetura-de-um-sistema-de-recuperacao-de-informacao}

Nesta seção é explicada a arquitetura de alto nível de um IRS. Além disso, é descrito o fluxo de dados do IRS, de acordo com as descrições dos IRMs estudados.
Acima foram detalhadas a forma que cada um dos IRMs estudados compreende a representação e recuperação de documentos.
Entretanto, nada foi dito sobre os diferentes componentes de um IRS e como esses componentes conversam entre si para resultar na recuperação da informação desejada.
Nesta seção, descreve-se o funcionamento geral de um IRS, ao passo que são descritas as particularidades dos IRSs, cuja implementação será descrita no próximo capítulo.

Como discutido anteriormente, diferentes IRMs atendem a propósitos distintos. Eles variam conforme as características dos documentos, as especificidades das consultas feitas pelos usuários e a estratégia adotada para recuperar e ranquear documentos na coleção.

Analogamente, a arquitetura de um IRS é moldada pelos IRMs que servem como fundamentação teórica, bem como pelos aspectos práticos considerados no desenvolvimento de software.

Ainda assim, é possível identificar elementos
comuns nas arquiteturas desses sistemas.
Os elementos comuns são discutidos a seguir, denominados componentes do sistema, conforme demonstrado na figura \ref{fig:irs-arquitetura},
em que as setas representam o fluxo de dados.

\begin{figure}
    \centering
    \includegraphics[width=1.0\textwidth]{figuras/arq-geral.png}
    \caption{Arquitetura Básica de um Sistema de Recuperação de Informação}
    \label{fig:irs-arquitetura}
\end{figure}

A coleta e estruturação de dados é o primeiro passo no processo de construção de um IRS e é realizada pelo Componente de Ingestão dos Dados.
Isso envolve a coleta de dados de várias fontes, como a \textit{web}, bases de dados, sistemas de arquivos, entre outros.
Os \textit{crawlers} são geralmente utilizados para rastrear e baixar o conteúdo da \textit{web}.

Uma vez coletados, os dados são levados ao Componente de Processamento para limpeza e estruturação dos dados. Esse procesasmento é realizado para extração de informações relevantes e remoção de informações irrelevantes, como uma preparação para a próxima etapa, de indexação. Esse processamento envolve técnicas PNL, como estematização, lematização, tokenização, entre outras.

O processo de organização dos dados estruturados de forma que possibilite a busca eficiente é chamado de indexação,
realizada pelo Componente de Indexação.
Nesse componente, os documentos preparados e armazenados em uma estrutura chamada índice, que permite ao sistema recuperar
rapidamente os documentos relevantes.

É no Componente de Consulta onde a consulta é executada e processada para recuperação dos documentos no índice.
É aqui onde são implementados algoritmos para determinar os resultados mais relevantes para a consulta.
Os resultados podem, ainda, depois de recuperados, ser ranqueados para então ser retornados ao usuário.

É comum que outros componentes sejam mencionados em discussões sobre arquitetura de sistemas de recuperação \cite{Ceri2013}. A escolha de ressaltar os componentes acima, da forma que foi feita, é uma escolha que faz sentido para o presente trabalho e para a discussão dos sistemas que se pretende comparar.

Especificamente para o projeto proposto nesse estudo, os componente mais relevantes para a discussão dos IRMs propostos são o Componente de Indexação, responsável criação e armazenamento das representações de documentos, e o Componente de Recuperação, responsável pela forma que a relevância dos documentos é computada.

Isso porque a forma que cada um dos modelos propõe resolver o problema do ranqueamento é fundamentalmente diferente.

%Ambos são componentes que responsáveis pela qualidade da consulta e pela velocidade com que consulta é efetuada.

Para implementação de um IRS baseado no IRM de vetores esparsos, a representação do documento é computada com base em uma matriz de incidência e armazenada em um índice invertido. Um índice invertido permite rápida recuperação de todos os documentos em que um termo está presente, através da busca do termo. Nesse sentido, um índice invertido se assemelha a um glossário, em que se buscam posições de uma palavra em um texto. No caso do índice invertido, frequência das palavras nos documentos.

A recuperação nesse tipo de sistema é realizada após o recebimento da consulta de um usuário, processamento dessa consulta, criação de uma representação vetorial esparsa, de forma idêntica à representação de um documento, e uma medida de associação, como o BM25, é computada para todos os documentos em que estão presentes os termos da consulta.
Em seguida, é realizado o ranqueamento por meio da comparação dos resultados obtidos pela medida de associação e os resultados ranqueados são retornados ao usuário.

Para implementação de um IRS baseado no IRM de vetores densos, a representação densa dos documentos é computada por um modelo de rede neural e armazenada em um depósito de vetores. Essas representações, embora mais ricas em informações e nuances contextuais, apresentam o desafio da alta dimensionalidade, tornando a consulta vetorial uma operação computacionalmente custosa.
Para superar esse obstáculo, técnicas como o \textit{Hierarchical Navigable Small World} (HNSW) são empregadas, que opera algoritmos para busca aproximada.

Após o recebimento da consulta de um usuário e processamento dessa consulta, obtém-se a representação densa da consulta e computa-se a representação densa da consulta, pelo mesmo modelo de rede neural usado para criar as representações dos documentos. Em seguida, são computadas medidas de associação como produto interno ou o cosseno dos vetores, para alguns documentos, de acordo com o algoritmo de HNSW. Os valores dessas medidas são usados no ranqueamento dos resultados que serão retornados ao usuário.

Implementados os sistemas, cumpre compreender como tais sistemas devem ser avaliados. Principalmente, como são avaliados o ranqueamento desses sistemas.

\section{Avaliando a Qualidade de um IRM: Métricas de Ranqueamento}\label{sec:metricas-de-ranqueamento}

A relevância é o principal critério para avaliar a qualidade do ranqueamento.
Sua natureza subjetiva e dinâmica requer avaliação dentro do contexto específico do sistema de recuperação de informação,
com base em métricas predefinidas.
Portanto, necessita-se saber aprioristicamente a relevância de um documento para uma consulta.
Isso é feito por meio de julgamentos de relevância.

Julgamentos de relevância (\textit{relevance judgments}) são avaliações humanas da relevância de uma coleção de texto para uma consulta.
Julgamentos de relevância servem a dois propósitos, podem ser usados para treinar um modelo de ranqueamento,
ou podem ser usados para avaliar funções de ranqueamento \cite{biblia}.

Julgamentos de relevância são também chamados de qrels e representados por um conjunto de triplas $(q, d, r)$, onde $q$ é uma consulta, $d$ é um documento e $rel(q, d) = r$
é um julgamento de relevância do documento $d$ para uma consulta $q$.
Em geral, $rel(q, d) = r \in \{0, 1, 2, 3, 4\}$, onde $r = 0$ significa que $d$ não possui qualquer relevância para $q$ e $r = 4$ significa que
$d$ é muito relevante para $q$.

Em um problema de ranqueamento, a ordem é relevante porque se presume que o usuário não consumirá muito mais do que
10 ou 20 resultados para sua busca.
Esse argumento é apoiado por diversas pesquisas que afirmam que o usuário de um mecanismo de busca de páginas web dificilmente vai além da 2 página \cite{biblia}.

É comum, portanto, apresentar uma métrica com uma lista $R$ cortada de 10 ou 20 resultados mais relevantes retornados pela consulta.
Quando isso acontece, é comum representar com a notação Métrica@$k$, onde $k$ é o tamanho da lista $R$ truncada \cite{biblia}.

Quando a ordem dos documentos recuperados é irrerelevante,
ou seja, quando a relevância de um documento é tratada como binária,
métricas usadas na avaliação da relevância da
informação são precisão e recall, ou variações estatísticas dessas.
Precisão avalia a ``solidez'' do sistema, provendo uma medida para a proporção de documentos relevantes dentre os
recuperados. Precisão é definida por:

\begin{equation}
    \text{Precisão}(R,q) = \frac{\sum_{(i,d)\in R} \text{rel}(q,d)}{|R|}
\end{equation}

Recall avalia a ``completude'' do sistema, provendo uma medida para a proporção de documentos relevantes que não
foram recuperados. Recall é definida por:

\begin{equation}
    \text{Recall}(R,q) = \frac{\sum_{(i,d)\in R} \text{rel}(q,d)}{\sum_{d\in C} \text{rel}(q,d)}
\end{equation}

% Sejam \textit{true positives} (TP) os documentos relevantes e recuperados,
% \textit{false positives} (FP) os que não são relevantes e foram recuperados,
% \textit{true negatives} (TN) os que não são relevantes e não foram recuperados,
% \textit{false negatives} (FN) os documentos que são relevantes e não foram recuperados,
% então precisão e recall podem ser descritos como
% $$P = \frac{TP}{TP + FP}$$
% $$R = \frac{TP}{TP + FN}$$
% Um sistema ideal deveria ter alta precisão e alto recall, próximos de 1, minimizando o número de FP e FN.
% Entretanto, isso nem sempre é possível, já que geralmente existe um \textit{trade-off} entre precisão e recall. [5 p.8]
% As métricas apresentadas acima não levam em consideração a ordem dos documentos retornados, e funcionam bem para avaliar sistemas que retornam conjuntos de documentos em qualquer ordem.
% Nem todos os sistemas de recuperação de informação se inserem nesse contexto.
% Para alguns sistemas, a ordem, ou \textit{rank}, é uma qualidade importante para a recuperação.
% Nesses casos, outras métricas devem ser consideradas, como o ganho de precisão ao aumentar a recall, \textit{Average Precision} (AP), ou a precisão em relação aos primeiros k documentos, \textit{Precision at k} (P@k).
% É isso que a precisão média calcula
% $$\text{AP} = \frac{1}{\text{R}} \sum_{k=1}^n P(k) \times \text{rel}(k)$$
% A precisão média calcula a precisão ao longo de diferentes k-cortes do resultado, somando e obtendo a média das precisões obtidas.

É possível, ainda, balancear ambas as métricas com uma média harmônica, métrica chamada de \textit{F-measure} ou com Precisão Média (\textit{AP -- Average Precision}).

Para o problema de ranqueamento, a métrica mais relevante é o Ganho Acumulado com Desconto Normalizado (\textit{nDCG -- Normalized Discounted Cumulative Gain)}, métrica ``especificamente desenvolvida para julgamentos de relevância graduados''.\cite{}

O Ganho Acumulado com Desconto (\textit{DCG -- Discounted Cumulative Gain}) para uma lista ranqueada $R = {(d_i, d_i)}$ é definido como:

\begin{equation}
    \text{DCG}(R,q) = \sum_{(i,d)\in R}\frac{2^{rel(q,d)} - 1}{\log_{2}(i+1)}
\end{equation}

A DCG é capaz de medir um ganho baseado na posição do documento recuperado por uma consulta, premiando quando um documento relevante aparece no topo da busca e penalizando quando aparece no fim da busca.
Isso porque, quanto maior $i$, maior o desconto sobre o numerador que computa a relevância.

O nDCG é definido por:

\begin{equation}
    \text{nDCG}(R,q) = \frac{\text{DCG}(R,q)}{\text{IDCG}(R,q)}
\end{equation}

onde IDCG representa o valor DCG para $R$ ideal para uma consulta $q$.
Assim, nDCG é um valor entre [0, 1], uma vez que DCG $\leq$ nDCG.

Essas métricas estão eficientemente implementadas na biblioteca trec\_eval
\footnote{https://github.com/usnistgov/trec\_eval}
, biblioteca bastante usada por
pesquisadores da área de recuperação de informação.

Algumas considerações devem ser feitas sobre sistemas especificos para recuperacao de literatura academica.

% Ler: https://arxiv.org/pdf/2104.08663.pdf
% https://arxiv.org/pdf/2206.12993.pdf

% Ler: https://medium.com/@readsumant/understanding-ndcg-as-a-metric-for-your-recomendation-system-5cd012fb3397
%      https://www.youtube.com/watch?v=EnlLr6S7c5A
%      https://towardsdatascience.com/demystifying-ndcg-bee3be58cfe0
%      https://mariofilho.com/ndcg-normalized-discounted-cumulative-gain-em-machine-learning/


\chapter{Implementação do Sistema de Recuperação de Informação de Teses}
\label{ch:implementacao-do-sistema-de-recuperacao-de-informacao-de-teses}

O problema inerente aos algoritmos clássicos de recuperação de informação que se baseiam no tf-idf ou no BM-25 é que a
pesquisa deve ser conduzida utilizando as palavras exatas presentes no texto que se busca.
Essa abordagem é problemática, pois o usuário do sistema pode compreender o que deseja buscar sem, necessariamente,
conhecer as palavras específicas que constam no documento alvo.

Conforme discutido em \cite{JurafskyMartin2023}, uma alternativa promissora a esse dilema é a utilização de vetores densos (embeddings),
em contraste com os vetores esparsos tradicionais.
Essa estratégia tem sido explorada tanto por acadêmicos quanto por empresas em diversos artigos publicados.
Ao empregar modelos modernos para codificar a consulta e os documentos, e ao realizar um produto vetorial
subsequente para calcular uma pontuação de similaridade, é possível oferecer resultados mais afinados com a intenção do usuário.

As arquiteturas dos sistemas de recuperação variam bastante, principalmente quanto ao IRM usado.
Pode-se citar alguns tipos diferentes de IRM, mesmo baseados em modelos de rede neurais.
Arquiteturas de vários estágios foram exploradas no passado, em que se utilizavam modelos diferentes para comparar e para ranquear o texto.
No meu caso, como já explicado, exploro exclusivamente

Por fim, cumpre ressaltar que a implementação de embeddings na busca ainda representa um desafio em aberto.
As áreas que despertam maior interesse incluem o ajuste preciso dos modelos para aumentar a relevância dos documentos
recuperados e a questão de como armazenar e recuperar vetores densos de maneira eficaz \cite{JurafskyMartin2023}.

A discussão sobre este novo paradigma de busca semântica, que emprega embeddings,
oscila principalmente entre duas características fundamentais: qualidade e eficiência.
Estas são, frequentemente, qualidades conflitantes.
Para alcançar uma qualidade superior,
o sistema requer uma busca mais abrangente, o que tende a aumentar a complexidade e reduzir a eficiência.
Por outro lado, para garantir eficiência, a busca de vetores densos precisa ser realizada de maneira aproximada,
o que pode comprometer a qualidade.
Passo a discutir os detalhes dessa implementação e, ao fim, mostrar os resultados.
Aproveito este capítulo para introduzir reflexões sobre robustês, sensibilidade e eficiência na implementação de um IRS.

% O autor \citeauthor{yan2021system} apresentou exemplos de arquiteturas para sistemas de recomendação
% \footnote{O desafio de arquitetar sistemas de busca está relacionado ao de arquitetar sistemas de recomendação.
% Ambos são exemplos de sistemas de recuperação de informação e são categorizados como ``discovery system: recommendation and search'' \cite{yan2021system}.}
% e de busca
% utilizadas na indústria,
% dividindo-as conforme os padrões: (a) Obter embeddings e construir um índice ANN ou um grafo de conhecimento para
% encontrar itens similares; (b) Ranquear esses embeddings selecionados em um espaço menor, de acordo com outro modelo
% ou heurísticas do negócio (por exemplo, o ranqueamento pode considerar características do usuário e histórico de busca
% em uma \textit{feature store} criada offline).

% Essa mesma arquitetura é recorrente em outros textos, mas ela pode contar com outros estágios como pós-filtragem ou múltiplas etapas de ranqueamento.
% Com isso, além dos modelos de ranqueamento, são adicionadas políticas
% específicas para filtrar e ordenar o resultado para cada usuário. Isso permite melhorar o sistema de busca,
% sem depender exclusivamente do modelo de recuperação ou ranqueamento.

% Alguns exemplos desse tipo de sistema incluem o sistema de recomendação do Instagram e a linguagem IGQL, e a
% arquitetura de sistema do Quora, que segue a divisão de Retrieval, Filtering, Scoring e Ordering.

% No meu caso, proponho uma arquitetura baseada unicamente na utilização de vetores densos.

% Para compreender melhor como esses sistemas funcionam, estudei [3], que explica, em alto nível, como funciona a arquitetura de busca do Semantic Scholar. Basicamente, a query passa para a Elasticsearch (AWS), e os 1000 primeiros resultados são ranqueados pelo ranker, usando um ranker LightGBM com um objetivo LambdaRank.

% Na minha arquitetura, devo considerar o que é importante em cada etapa. Quero desenvolver um sistema que busque rapidamente teses, sem necessidade de criar novos embeddings ou índices rapidamente, então posso priorizar a velocidade de consulta. Algumas bases de dados podem ser otimizadas em diferentes níveis, dependendo do momento do sistema de busca.

% Em uma visão geral, destacam-se três componentes de um sistema de recuperação de informação: um módulo de obtenção da consulta, um módulo de análise da consulta e busca, e um módulo de administração do conteúdo. Estes módulos são uma visão geral e podem conter subdivisões, como um sistema de ingestão de dados, um parseador, etc.

% [1]
% }
% ```
% [2] Recommender Systems, Not Just Recommender Models https://medium.com/nvidia-merlin/recommender-systems-not-just-recommender-models-485c161c755e
% [3] https://blog.allenai.org/building-a-better-search-engine-for-semantic-scholar-ea23a0b661e7

\section{Arquitetura do Sistema}\label{sec:arquitetura-do-sistema}

\begin{figure}
    \centering
    \includegraphics[scale=0.6]{figuras/minha-arq.png}
    \caption{Arquitetura do Sistema}
    \label{fig:enter-label}
\end{figure}

Exploro a arquitetura e os aspectos fundamentais de um sistema de busca inovador baseado em recuperação profunda. O sistema é estruturado em três componentes principais que garantem a eficácia e a eficiência do processo de busca.

O componente de ingestão é a porta de entrada para os dados, este componente função de coleta e limpeza dos documentos. Utiliza um \textit{crawler} para navegar e coletar informações de diversas fontes. Após a coleta, um processador de documentos atua na preparação e na normalização dos dados para que os dados relevantes possam ser recuperados de forma ágil e consistente, garantindo a integridade e a relevância do conteúdo que será pesquisado.

O componente de indexação é onde os dados coletados são transformados em um formato que pode ser eficientemente consultado. O tokenizador é responsável por uma segmentação dos documentos em unidades lógicas que são codificadas pelo modelo de rede neural. Foram testadas diferentes unidades lógicas, como segmentação em sentença, parágrafo e sentenças com palavras-chave. O codificador e o indexador trabalham em conjunto para codificar os tokens e armazenar essas codificações em um depósito de vetores.

É no componente de busca onde as consultas são realizadas e os resultados são apresentados. O buscador utiliza algoritmos avançados para vasculhar o índice e retornar os documentos mais relevantes com base nos termos da pesquisa. A eficácia deste componente é crítica, pois determina a qualidade da experiência do usuário ao utilizar o sistema de busca.

Cada um dos processos que rodam nos componentes persistem os dados em uma base de dados relacional. Dessa forma, eu garanto que todas as etapas são persistidas em disco e não são perdidas durante a execução. Essa abordagem também é uma forma de orquestrar a execução dos processos paralelamente, sem precisar me preocupar em concorrência, deixando que a base de dados cuide disso.

Por fim, cada um desses componentes desempenha um papel vital no funcionamento geral do sistema, trabalhando em harmonia para fornecer resultados rápidos e precisos.

Avançamos agora para uma análise mais detalhada do código e dos algoritmos que impulsionam este sistema de busca de ponta.


\section{Componente de Ingestão de Dados}\label{sec:ingestao-de-dados}
% ---------------------------------------------------------------------------------------------------------------------
% TÓPICOS:
% - Falar sobre o Crawler e sobre o repositório de teses da usp
% - Document parsing -> reconhecimento e estruturação;
% - Análise léxica
% - Remoção de stop words????
% (Não mais que uma página)
% ---------------------------------------------------------------------------------------------------------------------
% ---------------------------------------------------------------------------------------------------------------------

O componente de ingestão do sistema é iniciado pelo crawler, que é uma ferramenta assíncrona projetada para navegar e coletar dados de sitemaps, no caso o da Universidade de São Paulo (USP). Este código Python utiliza bibliotecas como requests para fazer requisições HTTP e BeautifulSoup para o parsing de XML, permitindo assim a extração de URLs de documentos a serem processados.

A classe RawData desempenha um papel crucial no funcionamento do \textit{crawler}, lidando especificamente com a coleta e estruturação dos metadados de teses de doutorado e dissertações de mestrado. A estratégia de coleta é refinada para se concentrar em páginas que contêm metadados, como indicado pela presença de um padrão específico na URL, e a classe é equipada com uma exceção personalizada para lidar com casos em que a URL fornecida não se enquadra neste critério.
Especificamente o título, o resumo e as palavras-chave das teses e dissertações são os documentos indexados no meu sistema de busca.

Durante o processamento dos documentos, foram encontrados desafios significativos relacionados à consistência dos dados. Isso incluiu a presença de strings indesejáveis que indicavam ausência de informação, como "Não disponível", a ocorrência de campos de resumos, títulos e palavras-chave vazios ou duplicados, e a falta de padronização na listagem de palavras-chave, com diferentes separadores e variações de caixa. Tais inconsistências podem afetar negativamente a eficiência da indexação e a precisão dos resultados de busca, sendo crucial implementar um processamento rigoroso para assegurar a uniformidade e a qualidade dos dados indexáveis.

Além disso, registrei situações onde obras não foram encontradas, ou urls diferentes que continham o mesmo trabalho ou mesmo que não continham  e foram identificadas inconsistências nos dados de alguns exceções e erros na etapa de coleta de dados.

O processador de texto trata da padronização e limpeza dos documentos. Essa classe elimina strings irrelevantes, campos vazios, duplicatas e formatação inconsistente de palavras-chave. As funcionalidades incluem a conversão de lotes de dados brutos em dataframes do pandas, remoção de registros não desejados ou duplicados, e a normalização de palavras-chave. A limpeza detalhada prepara os dados para a indexação, essencial para a eficácia do sistema de busca baseado em recuperação profunda. Métodos como clean\_keywords, mask\_problematic\_punctuation, e unmask\_problematic\_punctuation são usados para garantir a consistência dos dados. O processamento em lotes e o tratamento de exceções são automatizados para manutenção contínua da base de dados, uma prática comum em sistemas que lidam com grandes volumes de dados.

Casos em que os dados de título ou resumo eram inexistentes foram simplesmente removidos do conjunto de dados.

\section{Componente de Indexação}\label{sec:indexacao}

O Tokenizer desempenha uma função crítica no pré-processamento de texto, segmentando documentos em unidades lógicas - tokens - que são posteriormente utilizadas para codificação por um modelo de linguagem de rede neural. O objetivo da tokenização é transformar texto bruto em estruturas padronizadas que um modelo de linguagem pode interpretar eficientemente. Estes tokens podem ser sentenças completas ou palavras-chave isoladas, que capturam elementos essenciais do texto para que o modelo de linguagem possa captar a semântica de cada unidade lógica e realizar buscas precisas dentro do conjunto de dados. O processo inclui a identificação e separação correta de sentenças, além da extração e individualização de palavras-chave, preparando assim os dados para a etapa de codificação neural.

Este componente é adaptado para extrair unidades lógicas menores de texto, como sentenças individuais, devido a evidências de que a qualidade dos embeddings gerados por modelos de linguagem neural tende a diminuir com segmentos de texto maiores. Este fenômeno ocorre porque, conforme o texto se alonga, torna-se mais difícil para o modelo capturar e manter as nuances semânticas em um espaço de representação densa. Para mitigar esse problema e enriquecer o contexto semântico de unidades menores, o tokenizador incorpora palavras-chave junto às sentenças. Esse procedimento permite que, mesmo sentenças com carga semântica limitada, quando isoladas, possam ser relacionadas ao conteúdo maior do documento, potencializando a criação de embeddings mais ricos e significativos no espaço semântico do modelo.



% ---------------------------------------------------------------------------------------------------------------------
% NOTAS:
% \cite{Hiemstra2009} discute, em p. 7, dificuldades da implementação do modelo de espaço vetorial. Isso porque o cálculo
% da similaridade de cosseno depende de todos os componentes da representação vetorial. Entretanto, isso não está disponível
% em um índice invertido. Na prática, os valores normalizados e o produto vetorial precisam ser usados. Então, ou eles são
% adicionados em um
%
% No podcast [...], o entrevistado discute: In memory index vs disk index → HMSW INDEX (good trade off but memory
% hungry) — memmap in qdrant (good inn performance) knn, qdrant is the best in disk. Hybrid search. Pre filtering vs
% post filtering. Self hosted vs cloud.
% ---------------------------------------------------------------------------------------------------------------------

% ---------------------------------------------------------------------------------------------------------------------
% TÓPICOS:
% Por que usar um índice.
% Quais são os tipos de índices usados.
% Quais os índices usados no meu sistema.
% - ANN (approximate nearest neighbor): an algorithm that uses distance algorithms to locate nearby vectors.
% - kNN: an algorithm that uses proximity to make predictions about grouping.
% - (SPTAG) Space partition tree and graph: a library for large scale approximate nearest neighbors.
% - Faiss: Facebook’s similarity search algorithm.
% - HNSW (hierarchical navigable small world): a multilayered graph approach for determining similarity.
% ---------------------------------------------------------------------------------------------------------------------

\section{Busca}\label{sec:busca}



\section{Experimentos Para Avaliação do Sistema de Recuperação de Informação de Teses}\label{sec:experimentos-para-avaliacao-do-sistema-de-recuperacao-de-informacao-de-teses}

A classe Evaluator é responsável pela avaliação de um sistema de recuperação de informação de teses. Ela realiza essa avaliação executando consultas armazenadas e comparando os resultados obtidos com um conjunto de relevâncias pré-definido (qrels). Para cada consulta, o avaliador calcula duas métricas padrão em sistemas de recuperação de informação: Precisão e NDCG.

A precisão é calculada como a proporção de documentos relevantes recuperados entre os top-k resultados da busca. O NDCG é uma métrica que pondera a classificação dos resultados recuperados, dando mais importância para os hits que aparecem no topo da lista, e ajustando-os pela relevância ideal dos documentos baseada no qrels.

O método evaluate itera sobre cada consulta, executando a busca e calculando as métricas mencionadas. Após isso, os resultados são inseridos no banco de dados para futura análise e comparação de desempenho entre diferentes configurações do sistema de busca. O método measures é quem computa efetivamente o valor da precisão e do NDCG para um conjunto de hits de busca e um conjunto de qrels ideais. Estas métricas são essenciais para entender o quão bem o sistema está performando e onde ele pode ser melhorado.

O sistema desenvolvido oferece uma plataforma de busca e anotação para coletar avaliações de relevância (qrels) de usuários. Ele permite que os usuários realizem buscas e, em seguida, avaliem os documentos recuperados, fornecendo dados valiosos que podem ser usados para avaliar e aprimorar o desempenho do sistema de recuperação de informação.

Para a busca, o sistema utiliza mais de um tipo de buscador: um buscador neural e um buscador de repositório, que são acessados de forma assíncrona para recuperar e compilar resultados relevantes. O buscador neural, em particular, aproveita modelos de aprendizado profundo para entender e corresponder às consultas do usuário com os documentos mais pertinentes.

No que diz respeito à coleta de qrels, o sistema possui um endpoint, que é usado pelos usuários para enviar suas avaliações de relevância para documentos específicos. Esse processo não só ajuda a entender a eficácia do sistema de busca, mas também fornece dados que podem ser utilizados para treinar e melhorar os algoritmos de busca neural, garantindo que os resultados sejam cada vez mais alinhados com as necessidades e preferências dos usuários. Limites de taxa são impostos para evitar a sobrecarga do sistema.

Por fim,
tanto pesquisadores \cite{}, quanto a indústria \cite{}, têm cada vez mais usado comparações entre representações densas de texto para ranquear documentos a partir de uma consulta.
Ao fim, aprender representações semânticas de um texto é importante para lidar com os problemas de incompatibilidade de vocabulário ou polissemia, mas usar comparações por termos também é importante para lidar com termos raros ou intenções (\textit{intents}). \cite{bhaskar-craswell-2018}
De acordo com \citeauthor{bhaskar-craswell-2018}, ``na prática um modelo de IR deve sopesar a correspondência exata e inexata para um termo consultado''. Para esses autores, a decisão de que forma de correspondência usar para uma consulta deve considerar o contexto.
É isso que passo a verificar no próximo capítulo.

\chapter{Experimentos}
\label{ch:exper}
% ---------------------------------------------------------------------------------------------------------------------
% NOTAS
% -----
% Positioning the Query in the Vector Space
% Uma coisa para manter em mente é uma forma de adaptação de indexação por feedback descrita
% em \cite{Hiemstra2009} p.7. Um algoritmo de relevance feedback sugerido por Rocchio, em que um vetor é adaptado de
% acordo com o resultado da busca dos documentos e do resultado de relevância calculado .
% ---------------------------------------------------------------------------------------------------------------------

Neste capítulo, descrevo o processo de desenvolvimento e adaptação do sistema com base em um \textit{feedback loop}.
Depois do desenvolvimento de um sistema modular como descrito no Capítulo~\ref{ch:mecanismo-de-busca-semantica}, torna-se
fácil configurar os componentes do sistema para ajustá-los com base em resultados de experimentos.
Os experimentos tiveram um objetivo de entender o funcionamento corrente do sistema e ajustar seus componentes
para melhorar sua eficiência, em termos de relevância dos documentos retornados em uma consulta (\textbf{qualidade})
e em termos de \textbf{velocidade} para retornar tais documentos.

Inicialmente, comparo a eficiência de diferentes abordagens de implementação dos componentes do sistema de recuperação
de informação com dados de teses obtidas no repositório de teses da Universidade de São Paulo.
Em seguida, descrevo um experimento usado para comparar o novo sistema com o sistema de recuperação usado no \textit{website}
do repositório de teses da Universidade de São Paulo.

\section{Melhorando a Indexação}
No primeiro experimento que realizei, tentei usar toda a informação de alguns .pdfs selecionados das teses para criar
embeddings.
Além de ter sido demorado para criar cada um embedding, a criação de um embedding por documento se mostrou insatisfatória
para a busca, dada que muita informação era perdida, além de que o documento tinha muita sujeira, o que exige trabalho
extra de limpeza manual.
Essa abordagem resultou em um sistema lento para ser criado e trabalhoso.
Ou seja, não resultou em um bom sistema para fazer a busca.

No segundo experimento que realizei, usei apenas as informações de metadados contidas no site de teses, sem utilizar a tese em si. Os metadados são título, resumo e palavras-chave. Juntei esses 3 em um texto só para gerar cada embedding. O tempo de criação de tudo é em torno de alguns minutos. Com isso, fui capaz de criar um sistema de busca que abarcava todas as teses do site de tese da USP. O sistema foi capaz de obter resultados em menos de 1 segundo. Entretanto, notou-se que a busca foi insatisfatória em algumas instâncias.

Para demonstrar isso, estabeleci o texto de busca "ddos attack". A busca encontrou o vetor mais similar em 0.19s e o resultado retornado foi:

1. Title:  Mitigating DDoS attacks on IoT through machine learning and network functions virtualization
Author:  Oliveira, Guilherme Werneck de
Minha suposição é de que, se esse valor retornado é o mais relevante para "ddos attack", ele deve também ser o mais relevante, ou pelo menos perder para algum mais relevante ainda, para "ddos attack and machine learning". Mas não foi isso que aconteceu, ao buscar por "ddos attack and machine learning", a busca encontrou:

1. Title:  Machine learning in complex networks
Author:  Breve, Fabricio Aparecido
2. Title:  Architecture and development of a real-time multiple content generator system for video games
Author:  Pereira, Leonardo Tortoro
3. Title:  Performance prediction of application executed on GPUs using a simple analytical model and machine learning techniques
Author:  González, Marcos Tulio Amarís
4. Title:  Mitigating DDoS attacks on IoT through machine learning and network functions virtualization
Author:  Oliveira, Guilherme Werneck de
Mesmo que sem utilizar métodos formais de avaliação, nota-se que o sistema de busca não foi relevante ao buscar por "ddos attack and machine learning". Minha suposição é de que muita informação era perdida ao criar um único embedding para todos os metadados.

No terceiro experimento que realizei, utilizei os mesmos metadados, mas decidi criar os embeddings a partir de cada sentença, e não usando todos os metadados ao mesmo tempo. Essa abordagem resultou em buscas em torno de 50 vezes mais lentas, mas bem mais relevantes.

Considere novamente o texto "ddos attack". A busca demorou 4s e o resultado foi:

1. Title:  Method for mitigating against distributed denial of service attacks using multi-agent system.
Author:  Pereira, João Paulo Aragão (Catálogo USP)
2. Title:  A collaborative architecture against DDOS attacks for cloud computing systems.
Author:  Almeida, Thiago Rodrigues Meira de (Catálogo USP)
3. Title:  Mitigating DDoS attacks on IoT through machine learning and network functions virtualization
Author:  Oliveira, Guilherme Werneck de (Catálogo USP)
Comparado ao resultado anterior, eu consideraria que esses resultados foram tão relevantes quanto para essa busca genérica específica. Para saber mais, eu precisaria comparar os 15 primeiros resultados, por exemplo.

Como segundo teste, busquei o texto "ddos attack and machine learning" e o resultado foi:

1. Title:  Mitigating DDoS attacks on IoT through machine learning and network functions virtualization
Author:  Oliveira, Guilherme Werneck de (Catálogo USP)
2. Title:  Reconfigurable learning system for classification of data using parallel processing
Author:  Moreira, Eduardo Marmo (Catálogo USP)
3. Title:  A collaborative architecture against DDOS attacks for cloud computing systems.
Author:  Almeida, Thiago Rodrigues Meira de (Catálogo USP)
Ou seja, pode-se considerar que houve uma melhora considerável de relevância para essa busca específica.




\section{Medindo a qualidade do sistema}
% (NOTE: 1.2 em diante)
% Devem ser propostos experimentos, portanto, para formalizar a avaliação da qualidade da recuperação.
% Com esses experimentos, busca-se criar um conjunto de consultas e relacionar essas consultas a uma coleção de documentos que devem ser retornados.

\chapter*[Conclusão]{Conclusão}
\addcontentsline{toc}{chapter}{Conclusão}


% ----------------------------------------------------------
% ELEMENTOS PÓS-TEXTUAIS
% ----------------------------------------------------------
\postextual
% ----------------------------------------------------------

% ----------------------------------------------------------
% Glossário
% ----------------------------------------------------------
%
% Consulte o manual da classe abntex2 para orientações sobre o glossário.
%
%\glossary

% ----------------------------------------------------------
% Apêndices
% ----------------------------------------------------------
% % ---
% % Inicia os apêndices
% % ---
% \begin{apendicesenv}
% % Imprime uma página indicando o início dos apêndices
% \partapendices
% % ----------------------------------------------------------
% \chapter{Quisque libero justo}
% % ----------------------------------------------------------
% \lipsum[50]
% % ----------------------------------------------------------
% \chapter{Nullam elementum urna vel imperdiet sodales elit ipsum pharetra ligula
% ac pretium ante justo a nulla curabitur tristique arcu eu metus}
% % ----------------------------------------------------------
% \lipsum[55-57]
% \end{apendicesenv}
% % ---

% ----------------------------------------------------------
% Anexos
% ----------------------------------------------------------
% % ---
% % Inicia os anexos
% % ---
% \begin{anexosenv}
% % Imprime uma página indicando o início dos anexos
% \partanexos
% % ---
% \chapter{Morbi ultrices rutrum lorem.}
% % ---
% \lipsum[30]
% % ---
% \chapter{Cras non urna sed feugiat cum sociis natoque penatibus et magnis dis
% parturient montes nascetur ridiculus mus}
% % ---
% \lipsum[31]
% % ---
% \chapter{Fusce facilisis lacinia dui}
% % ---
% \lipsum[32]
% \end{anexosenv}

%---------------------------------------------------------------------
% INDICE REMISSIVO
%---------------------------------------------------------------------
%\phantompart
\printindex
%---------------------------------------------------------------------

\end{document}